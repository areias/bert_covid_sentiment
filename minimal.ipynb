{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minimal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMRbNYcbfB9JTeD3pqZUNy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/areias/bert_covid_sentiment/blob/main/minimal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJpl8lrrtqgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71fee2d-42dd-455a-c484-cb3eb9cfe87a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5d5hmYfZTCx",
        "outputId": "e01f1bd3-c6e2-4427-d03e-fe2f4f9766f9"
      },
      "source": [
        "!git clone -b master https://github.com/digitalepidemiologylab/covid-twitter-bert.git drive/MyDrive/covid-twitter-bert"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'drive/MyDrive/covid-twitter-bert'...\n",
            "remote: Enumerating objects: 1660, done.\u001b[K\n",
            "remote: Counting objects: 100% (222/222), done.\u001b[K\n",
            "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
            "remote: Total 1660 (delta 150), reused 140 (delta 88), pack-reused 1438\u001b[K\n",
            "Receiving objects: 100% (1660/1660), 3.48 MiB | 10.01 MiB/s, done.\n",
            "Resolving deltas: 100% (1050/1050), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTzMXnm0bbuo"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/models-93490036e00f37ecbe6693b9ff4ae488bb8e9270/* drive/MyDrive/covid-twitter-bert/tensorflow_models/"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WqFkXHLxrlq"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sExhCP4NwZTd",
        "outputId": "f24a91e6-213d-4373-a1b4-e70c67746e34"
      },
      "source": [
        "!pip install -r drive/MyDrive/covid-twitter-bert/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 3.5 kB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 3)) (0.12.0)\n",
            "Collecting tensorflow_addons==0.11.2\n",
            "  Downloading tensorflow_addons-0.11.2-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 5)) (4.62.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 6)) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (1.18.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (2.2.4)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 51.6 MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 52.4 MB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 39.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.37.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.2.0)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.17.3)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.42.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 51.1 MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 29.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons==0.11.2->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 4)) (2.7.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 6)) (2018.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (1.0.3)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (1.26.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (1.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (1.0.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 12)) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 12)) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 12)) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 12)) (0.0.4)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=52a440271cd989a8127b45be9624be118f25389406126ba677821659fdfcdc49\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, h5py, google-api-python-client, gast, unidecode, tensorflow-addons, tensorflow, sentencepiece, emoji, cloud-tpu-client\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.290 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 emoji-1.6.1 gast-0.3.3 google-api-python-client-1.8.0 h5py-2.10.0 sentencepiece-0.1.96 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-addons-0.11.2 tensorflow-estimator-2.2.0 unidecode-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbEpfTBt3vj_",
        "outputId": "67e2d5a3-2ee4-4b9d-8f60-774a85739378"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFH-g2woxu6L"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngmvl7_mxMDy"
      },
      "source": [
        "import pandas as pd \n",
        "df=pd.read_csv(\"/content/drive/MyDrive/train-4.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3xjz9o8xoc9"
      },
      "source": [
        "# id,label,text\n",
        "df=df.loc[:,[\"tweet_id\",\"label\", \"text\"]]\n",
        "df.columns=['id','label', 'text']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEUBCR4b40Ww",
        "outputId": "bb8eb428-c349-43ba-f458-30cabff85fe0"
      },
      "source": [
        "type(df['label'][0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2_pC8XN43oV"
      },
      "source": [
        "df['label']=df['label'].apply(lambda x: str(x))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYddW5wNzfD-",
        "outputId": "3e000eca-aeed-45b0-94f7-dd14fa45be84"
      },
      "source": [
        "type(df['id'][0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.float64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Sw7W6Rczj2Q"
      },
      "source": [
        "df['id']=df['id'].apply(lambda x: int(x))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh-d0iIKzqgJ",
        "outputId": "4f252639-968e-42f6-9724-063e1185cb73"
      },
      "source": [
        "type(df['id'][0])\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FvqnxvKV0D97",
        "outputId": "5beb8d10-f111-4001-9727-7d8dfad783cc"
      },
      "source": [
        "df['label'][0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQihXrYrynHt"
      },
      "source": [
        "import numpy as np\n",
        "\"\"\"\n",
        "    60% - train set,\n",
        "    20% - dev/validation set,\n",
        "    20% - test set\"\"\"\n",
        "\n",
        "train, dev, test = np.split(df.sample(frac=1, random_state=42), \n",
        "                       [int(.6*len(df)), int(.8*len(df))])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRKgc09J5NY-",
        "outputId": "2197a031-d7b7-4174-cd37-6dc81dbc2208"
      },
      "source": [
        "type(train['label'][0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF39d4iwzBJw"
      },
      "source": [
        "import os\n",
        "os.makedirs(\"drive/MyDrive/covid-twitter-bert/data/finetune/originals/crowdbreaks\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2HKIFdyuLQ"
      },
      "source": [
        "train.to_csv(\"drive/MyDrive/covid-twitter-bert/data/finetune/originals/crowdbreaks/train.tsv\", sep=\"\\t\",index=False)\n",
        "dev.to_csv(\"drive/MyDrive/covid-twitter-bert/data/finetune/originals/crowdbreaks/dev.tsv\", sep=\"\\t\",index=False)\n",
        "test.to_csv(\"drive/MyDrive/covid-twitter-bert/data/finetune/originals/crowdbreaks/test.tsv\", sep=\"\\t\",index=False)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiNfAwnw0n_w",
        "outputId": "fbc03011-b21e-4351-e257-7c27fbd1b5b5"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSS-qxD3TdA",
        "outputId": "f04bc1b1-42d1-4edd-8e5e-5f3d7016efe7"
      },
      "source": [
        "!pip install spacy==3"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==3\n",
            "  Downloading spacy-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (4.8.2)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "  Downloading pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (21.3)\n",
            "Collecting thinc<8.1.0,>=8.0.0\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 59.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (3.0.6)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (4.62.3)\n",
            "Collecting pathy\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (3.10.0.2)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (0.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy==3) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy==3) (5.2.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.1 pydantic-1.7.4 spacy-3.0.0 spacy-legacy-3.0.8 srsly-2.4.2 thinc-8.0.13 typer-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ7ebfVd2xvg",
        "outputId": "d1d3168c-26ca-448f-c8fb-9a6e9b0c4ab2"
      },
      "source": [
        "import spacy \n",
        "print(spacy.__version__)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caoFUCLmarcw"
      },
      "source": [
        "import sys\n",
        "sys.path.append('drive/MyDrive/covid-twitter-bert')\n",
        "sys.path.append('drive/MyDrive/covid-twitter-bert/tensorflow_models')\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjffDOVfy_9t",
        "outputId": "29fabaed-b48e-4fbf-82d8-b09bcefae1ef"
      },
      "source": [
        "# had to change line 462 again in ctbert/tensorflow_models/official/nlp/data/classifier_data_lib.py\n",
        "# also changed config.py line 32 to point to ct-bert model vs. 2\n",
        "%%shell \n",
        "cd drive/MyDrive/covid-twitter-bert/preprocess \n",
        "python create_finetune_data.py --run_prefix test_run --finetune_datasets crowdbreaks --model_class bert_large_uncased_wwm --max_seq_length 96 --asciify_emojis --username_filler twitteruser --url_filler twitterurl --replace_multiple_usernames --replace_multiple_urls --remove_unicode_symbols"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-24 14:56:07,013 [INFO ] [__main__    ]: Processing dataset crowdbreaks...\n",
            "2021-11-24 14:56:07,016 [INFO ] [__main__    ]: Reading data for for type train...\n",
            "2021-11-24 14:56:07,030 [INFO ] [__main__    ]: Creating preprocessed files...\n",
            "2021-11-24 14:56:08,456 [INFO ] [__main__    ]: Reading data for for type dev...\n",
            "2021-11-24 14:56:08,462 [INFO ] [__main__    ]: Creating preprocessed files...\n",
            "2021-11-24 14:56:08,899 [INFO ] [__main__    ]: Creating tfrecords files...\n",
            "2021-11-24 14:56:09,016 [INFO ] [absl        ]: Writing example 0 of 2041\n",
            "2021-11-24 14:56:09,017 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 14:56:09,017 [INFO ] [absl        ]: guid: train-0\n",
            "2021-11-24 14:56:09,017 [INFO ] [absl        ]: tokens: [CLS] arizona classrooms vulnerable to me ##as ##les outbreak : the arizona department of health services looked at herd . . . twitter ##ur ##l [SEP]\n",
            "2021-11-24 14:56:09,017 [INFO ] [absl        ]: input_ids: 101 5334 12463 8211 2000 2033 3022 4244 8293 1024 1996 5334 2533 1997 2740 2578 2246 2012 14906 1012 1012 1012 10474 3126 2140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,017 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,017 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,017 [INFO ] [absl        ]: label: 0 (id = 1)\n",
            "2021-11-24 14:56:09,018 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 14:56:09,018 [INFO ] [absl        ]: guid: train-1\n",
            "2021-11-24 14:56:09,018 [INFO ] [absl        ]: tokens: [CLS] about to shower & get ready early , i need to fa ##x some stuff to my doctor so they an fa ##x my im ##mun ##ization to my school [SEP]\n",
            "2021-11-24 14:56:09,018 [INFO ] [absl        ]: input_ids: 101 2055 2000 6457 1004 2131 3201 2220 1010 1045 2342 2000 6904 2595 2070 4933 2000 2026 3460 2061 2027 2019 6904 2595 2026 10047 23041 3989 2000 2026 2082 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,018 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,019 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,019 [INFO ] [absl        ]: label: 1 (id = 2)\n",
            "2021-11-24 14:56:09,019 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 14:56:09,019 [INFO ] [absl        ]: guid: train-2\n",
            "2021-11-24 14:56:09,019 [INFO ] [absl        ]: tokens: [CLS] i will never have kids . $ 350 for va ##cci ##nation ##s and check - ups . only jack and andy are worth it . # dog ##mo ##m # broke [SEP]\n",
            "2021-11-24 14:56:09,019 [INFO ] [absl        ]: input_ids: 101 1045 2097 2196 2031 4268 1012 1002 8698 2005 12436 14693 9323 2015 1998 4638 1011 11139 1012 2069 2990 1998 5557 2024 4276 2009 1012 1001 3899 5302 2213 1001 3631 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,020 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,020 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,020 [INFO ] [absl        ]: label: 1 (id = 2)\n",
            "2021-11-24 14:56:09,020 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 14:56:09,020 [INFO ] [absl        ]: guid: train-3\n",
            "2021-11-24 14:56:09,020 [INFO ] [absl        ]: tokens: [CLS] me ##as ##les outbreak includes 5 disney theme park employees twitter ##ur ##l [SEP]\n",
            "2021-11-24 14:56:09,020 [INFO ] [absl        ]: input_ids: 101 2033 3022 4244 8293 2950 1019 6373 4323 2380 5126 10474 3126 2140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,020 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,021 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,021 [INFO ] [absl        ]: label: 0 (id = 1)\n",
            "2021-11-24 14:56:09,021 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 14:56:09,021 [INFO ] [absl        ]: guid: train-4\n",
            "2021-11-24 14:56:09,021 [INFO ] [absl        ]: tokens: [CLS] via twitter ##user : australia to stop payments to families who refuse child va ##cci ##nation ##s twitter ##ur ##l [SEP]\n",
            "2021-11-24 14:56:09,021 [INFO ] [absl        ]: input_ids: 101 3081 10474 20330 1024 2660 2000 2644 10504 2000 2945 2040 10214 2775 12436 14693 9323 2015 10474 3126 2140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,021 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,022 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:09,022 [INFO ] [absl        ]: label: 0 (id = 1)\n",
            "2021-11-24 14:56:10,153 [INFO ] [absl        ]: Writing example 0 of 680\n",
            "2021-11-24 14:56:10,153 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 14:56:10,153 [INFO ] [absl        ]: guid: dev-0\n",
            "2021-11-24 14:56:10,153 [INFO ] [absl        ]: tokens: [CLS] flu shot , got . # va ##cci ##nated # flush ##ot # disease ##pre ##vent ##ion # vaccines ##work # flu ##zone # target . . . twitter ##ur ##l [SEP]\n",
            "2021-11-24 14:56:10,153 [INFO ] [absl        ]: input_ids: 101 19857 2915 1010 2288 1012 1001 12436 14693 23854 1001 13862 4140 1001 4295 28139 15338 3258 1001 28896 6198 1001 19857 15975 1001 4539 1012 1012 1012 10474 3126 2140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,154 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,154 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,154 [INFO ] [absl        ]: label: 1 (id = 2)\n",
            "2021-11-24 14:56:10,154 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 14:56:10,155 [INFO ] [absl        ]: guid: dev-1\n",
            "2021-11-24 14:56:10,155 [INFO ] [absl        ]: tokens: [CLS] idiots . twitter ##user : texas me ##as ##les outbreak traced to anti - vaccine mega ##church . twitter ##ur ##l [SEP]\n",
            "2021-11-24 14:56:10,155 [INFO ] [absl        ]: input_ids: 101 28781 1012 10474 20330 1024 3146 2033 3022 4244 8293 9551 2000 3424 1011 17404 13164 22743 1012 10474 3126 2140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,155 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,155 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,155 [INFO ] [absl        ]: label: 1 (id = 2)\n",
            "2021-11-24 14:56:10,156 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 14:56:10,156 [INFO ] [absl        ]: guid: dev-2\n",
            "2021-11-24 14:56:10,156 [INFO ] [absl        ]: tokens: [CLS] at the vet my baby is getting a va ##cci ##nation aw ##w ##w ##w ##w poor baby : - ( [SEP]\n",
            "2021-11-24 14:56:10,156 [INFO ] [absl        ]: input_ids: 101 2012 1996 29525 2026 3336 2003 2893 1037 12436 14693 9323 22091 2860 2860 2860 2860 3532 3336 1024 1011 1006 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,156 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,156 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,156 [INFO ] [absl        ]: label: 1 (id = 2)\n",
            "2021-11-24 14:56:10,157 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 14:56:10,157 [INFO ] [absl        ]: guid: dev-3\n",
            "2021-11-24 14:56:10,157 [INFO ] [absl        ]: tokens: [CLS] \" today in journalism my professor said friend ' s daughter got va ##cci ##nated , took a nap , woke , \" \" was never the same ; \" \" later diagnosed with autism \" [SEP]\n",
            "2021-11-24 14:56:10,157 [INFO ] [absl        ]: input_ids: 101 1000 2651 1999 8083 2026 2934 2056 2767 1005 1055 2684 2288 12436 14693 23854 1010 2165 1037 18996 1010 8271 1010 1000 1000 2001 2196 1996 2168 1025 1000 1000 2101 11441 2007 19465 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,157 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,157 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,157 [INFO ] [absl        ]: label: -1 (id = 0)\n",
            "2021-11-24 14:56:10,158 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 14:56:10,158 [INFO ] [absl        ]: guid: dev-4\n",
            "2021-11-24 14:56:10,158 [INFO ] [absl        ]: tokens: [CLS] twitter ##user and children getting va ##cci ##nated still risk side effects and death . [SEP]\n",
            "2021-11-24 14:56:10,158 [INFO ] [absl        ]: input_ids: 101 10474 20330 1998 2336 2893 12436 14693 23854 2145 3891 2217 3896 1998 2331 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,158 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,158 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 14:56:10,158 [INFO ] [absl        ]: label: -1 (id = 0)\n",
            "2021-11-24 14:56:10,536 [INFO ] [__main__    ]: Sucessfully wrote tfrecord files to ../data/finetune/run_2021-11-24_14-56-07_010193_test_run/crowdbreaks/tfrecords\n",
            "2021-11-24 14:56:10,538 [INFO ] [__main__    ]: Saving config to ../data/finetune/run_2021-11-24_14-56-07_010193_test_run/create_finetune_config.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HnJhV-fc8F2"
      },
      "source": [
        "!rm -r /content/drive/MyDrive/covid-twitter-bert/data/finetune/run_2021-11-24_14-55-26_453313_test_run"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raLl3-cv-3Wa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}