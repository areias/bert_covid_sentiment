{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minimal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyORer2tEdb4gTkHWrgcK6hE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/areias/bert_covid_sentiment/blob/main/minimal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJpl8lrrtqgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceef4291-b453-45b9-f605-d456b340d95c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WqFkXHLxrlq"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sExhCP4NwZTd",
        "outputId": "4f546403-4412-4b7f-f9b8-267556f1b7e6"
      },
      "source": [
        "!pip install -r drive/MyDrive/ct-bert/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 3.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/ct-bert/requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/ct-bert/requirements.txt (line 3)) (0.12.0)\n",
            "Collecting tensorflow_addons==0.11.2\n",
            "  Downloading tensorflow_addons-0.11.2-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/ct-bert/requirements.txt (line 5)) (4.62.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/ct-bert/requirements.txt (line 6)) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/ct-bert/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/ct-bert/requirements.txt (line 8)) (1.18.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/ct-bert/requirements.txt (line 9)) (2.2.4)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 76.5 MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 74.6 MB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 60.6 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 71.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.19.5)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.1.2)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.42.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (0.37.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.15.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (3.17.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons==0.11.2->-r drive/MyDrive/ct-bert/requirements.txt (line 4)) (2.7.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/ct-bert/requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r drive/MyDrive/ct-bert/requirements.txt (line 6)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r drive/MyDrive/ct-bert/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r drive/MyDrive/ct-bert/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r drive/MyDrive/ct-bert/requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->-r drive/MyDrive/ct-bert/requirements.txt (line 8)) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->-r drive/MyDrive/ct-bert/requirements.txt (line 8)) (1.0.3)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/ct-bert/requirements.txt (line 8)) (1.26.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/ct-bert/requirements.txt (line 8)) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/ct-bert/requirements.txt (line 8)) (1.53.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/ct-bert/requirements.txt (line 8)) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/ct-bert/requirements.txt (line 9)) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/ct-bert/requirements.txt (line 9)) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/ct-bert/requirements.txt (line 9)) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/ct-bert/requirements.txt (line 9)) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/ct-bert/requirements.txt (line 9)) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/ct-bert/requirements.txt (line 9)) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/ct-bert/requirements.txt (line 9)) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/ct-bert/requirements.txt (line 9)) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/ct-bert/requirements.txt (line 9)) (1.0.0)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client->-r drive/MyDrive/ct-bert/requirements.txt (line 12)) (4.1.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client->-r drive/MyDrive/ct-bert/requirements.txt (line 12)) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client->-r drive/MyDrive/ct-bert/requirements.txt (line 12)) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client->-r drive/MyDrive/ct-bert/requirements.txt (line 12)) (0.0.4)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=1e599d4465251b617cc65ce8f4040a124efdb54bbaefc9de6874a538997d0db6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, h5py, google-api-python-client, gast, unidecode, tensorflow-addons, tensorflow, sentencepiece, emoji, cloud-tpu-client\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.290 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 emoji-1.6.1 gast-0.3.3 google-api-python-client-1.8.0 h5py-2.10.0 sentencepiece-0.1.96 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-addons-0.11.2 tensorflow-estimator-2.2.0 unidecode-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbEpfTBt3vj_",
        "outputId": "0042783b-851e-44fa-eec0-0a0d0076ba61"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFH-g2woxu6L"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngmvl7_mxMDy"
      },
      "source": [
        "import pandas as pd \n",
        "df=pd.read_csv(\"/content/drive/MyDrive/train-4.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3xjz9o8xoc9"
      },
      "source": [
        "# id,label,text\n",
        "df=df.loc[:,[\"tweet_id\",\"label\", \"text\"]]\n",
        "df.columns=['id','label', 'text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEUBCR4b40Ww",
        "outputId": "7b947884-2579-47f2-e24c-812f804bff3b"
      },
      "source": [
        "type(df['label'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2_pC8XN43oV"
      },
      "source": [
        "df['label']=df['label'].apply(lambda x: str(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYddW5wNzfD-",
        "outputId": "848468d3-3009-4ae0-b368-d4364c4d9b28"
      },
      "source": [
        "type(df['id'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Sw7W6Rczj2Q"
      },
      "source": [
        "df['id']=df['id'].apply(lambda x: int(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh-d0iIKzqgJ",
        "outputId": "ebac16e2-1941-412b-8e3a-54be709ad556"
      },
      "source": [
        "type(df['id'][0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FvqnxvKV0D97",
        "outputId": "296701a6-b9b5-4183-f822-c319fa2ad2ec"
      },
      "source": [
        "df['label'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQihXrYrynHt"
      },
      "source": [
        "import numpy as np\n",
        "\"\"\"\n",
        "    60% - train set,\n",
        "    20% - dev/validation set,\n",
        "    20% - test set\"\"\"\n",
        "\n",
        "train, dev, test = np.split(df.sample(frac=1, random_state=42), \n",
        "                       [int(.6*len(df)), int(.8*len(df))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRKgc09J5NY-",
        "outputId": "3d7d6f20-b9c5-45d2-c584-bc48105ee8c9"
      },
      "source": [
        "type(train['label'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF39d4iwzBJw"
      },
      "source": [
        "import os\n",
        "os.makedirs(\"drive/MyDrive/ct-bert/data/finetune/originals/crowdbreaks\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2HKIFdyuLQ"
      },
      "source": [
        "train.to_csv(\"drive/MyDrive/ct-bert/data/finetune/originals/crowdbreaks/train.tsv\", sep=\"\\t\",index=False)\n",
        "dev.to_csv(\"drive/MyDrive/ct-bert/data/finetune/originals/crowdbreaks/dev.tsv\", sep=\"\\t\",index=False)\n",
        "test.to_csv(\"drive/MyDrive/ct-bert/data/finetune/originals/crowdbreaks/test.tsv\", sep=\"\\t\",index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiNfAwnw0n_w",
        "outputId": "c63e0f18-6956-4d3c-b608-d0d53afba070"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSS-qxD3TdA",
        "outputId": "19c4a104-492a-4be2-a54d-ac032eaa1355"
      },
      "source": [
        "!pip install spacy==3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==3\n",
            "  Downloading spacy-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (3.10.0.2)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (1.19.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (2.11.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (4.8.2)\n",
            "Collecting thinc<8.1.0,>=8.0.0\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 70.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (4.62.3)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (1.0.6)\n",
            "Collecting pathy\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (0.8.2)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3) (21.3)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "  Downloading pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 23.1 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.0\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy==3) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3) (3.0.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3) (3.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy==3) (5.2.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.1 pydantic-1.7.4 spacy-3.0.0 spacy-legacy-3.0.8 srsly-2.4.2 thinc-8.0.13 typer-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ7ebfVd2xvg",
        "outputId": "e81ad77b-dc19-4c1b-c9cd-278ff9059e22"
      },
      "source": [
        "import spacy \n",
        "print(spacy.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjffDOVfy_9t",
        "outputId": "93288be6-7d04-4d16-db21-5a9c5932494a"
      },
      "source": [
        "# had to change line 462 again in ctbert/tensorflow_models/official/nlp/data/classifier_data_lib.py\n",
        "# also changed config.py line 32 to point to ct-bert model vs. 2\n",
        "%%shell \n",
        "cd drive/MyDrive/ct-bert/preprocess \n",
        "python create_finetune_data.py --run_prefix test_run --finetune_datasets crowdbreaks --model_class bert_large_uncased_wwm --max_seq_length 96 --asciify_emojis --username_filler twitteruser --url_filler twitterurl --replace_multiple_usernames --replace_multiple_urls --remove_unicode_symbols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-24 08:24:14,635 [INFO ] [__main__    ]: Processing dataset crowdbreaks...\n",
            "2021-11-24 08:24:14,639 [INFO ] [__main__    ]: Reading data for for type train...\n",
            "2021-11-24 08:24:16,754 [INFO ] [__main__    ]: Creating preprocessed files...\n",
            "2021-11-24 08:24:18,401 [INFO ] [__main__    ]: Reading data for for type dev...\n",
            "2021-11-24 08:24:20,082 [INFO ] [__main__    ]: Creating preprocessed files...\n",
            "2021-11-24 08:24:20,646 [INFO ] [__main__    ]: Creating tfrecords files...\n",
            "2021-11-24 08:24:21,212 [INFO ] [absl        ]: Writing example 0 of 2041\n",
            "2021-11-24 08:24:21,212 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 08:24:21,212 [INFO ] [absl        ]: guid: train-0\n",
            "2021-11-24 08:24:21,213 [INFO ] [absl        ]: tokens: [CLS] arizona classrooms vulnerable to me ##as ##les outbreak : the arizona department of health services looked at herd . . . twitter ##ur ##l [SEP]\n",
            "2021-11-24 08:24:21,213 [INFO ] [absl        ]: input_ids: 101 5334 12463 8211 2000 2033 3022 4244 8293 1024 1996 5334 2533 1997 2740 2578 2246 2012 14906 1012 1012 1012 10474 3126 2140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,213 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,213 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,213 [INFO ] [absl        ]: label: 0 (id = 1)\n",
            "2021-11-24 08:24:21,213 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 08:24:21,213 [INFO ] [absl        ]: guid: train-1\n",
            "2021-11-24 08:24:21,214 [INFO ] [absl        ]: tokens: [CLS] about to shower & get ready early , i need to fa ##x some stuff to my doctor so they an fa ##x my im ##mun ##ization to my school [SEP]\n",
            "2021-11-24 08:24:21,214 [INFO ] [absl        ]: input_ids: 101 2055 2000 6457 1004 2131 3201 2220 1010 1045 2342 2000 6904 2595 2070 4933 2000 2026 3460 2061 2027 2019 6904 2595 2026 10047 23041 3989 2000 2026 2082 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,214 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,214 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,214 [INFO ] [absl        ]: label: 1 (id = 2)\n",
            "2021-11-24 08:24:21,214 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 08:24:21,214 [INFO ] [absl        ]: guid: train-2\n",
            "2021-11-24 08:24:21,214 [INFO ] [absl        ]: tokens: [CLS] i will never have kids . $ 350 for va ##cci ##nation ##s and check - ups . only jack and andy are worth it . # dog ##mo ##m # broke [SEP]\n",
            "2021-11-24 08:24:21,214 [INFO ] [absl        ]: input_ids: 101 1045 2097 2196 2031 4268 1012 1002 8698 2005 12436 14693 9323 2015 1998 4638 1011 11139 1012 2069 2990 1998 5557 2024 4276 2009 1012 1001 3899 5302 2213 1001 3631 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,214 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,215 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,215 [INFO ] [absl        ]: label: 1 (id = 2)\n",
            "2021-11-24 08:24:21,215 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 08:24:21,215 [INFO ] [absl        ]: guid: train-3\n",
            "2021-11-24 08:24:21,215 [INFO ] [absl        ]: tokens: [CLS] me ##as ##les outbreak includes 5 disney theme park employees twitter ##ur ##l [SEP]\n",
            "2021-11-24 08:24:21,215 [INFO ] [absl        ]: input_ids: 101 2033 3022 4244 8293 2950 1019 6373 4323 2380 5126 10474 3126 2140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,215 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,215 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,215 [INFO ] [absl        ]: label: 0 (id = 1)\n",
            "2021-11-24 08:24:21,216 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 08:24:21,216 [INFO ] [absl        ]: guid: train-4\n",
            "2021-11-24 08:24:21,216 [INFO ] [absl        ]: tokens: [CLS] via twitter ##user : australia to stop payments to families who refuse child va ##cci ##nation ##s twitter ##ur ##l [SEP]\n",
            "2021-11-24 08:24:21,216 [INFO ] [absl        ]: input_ids: 101 3081 10474 20330 1024 2660 2000 2644 10504 2000 2945 2040 10214 2775 12436 14693 9323 2015 10474 3126 2140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,216 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,216 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:21,216 [INFO ] [absl        ]: label: 0 (id = 1)\n",
            "2021-11-24 08:24:22,092 [INFO ] [absl        ]: Writing example 0 of 680\n",
            "2021-11-24 08:24:22,092 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 08:24:22,092 [INFO ] [absl        ]: guid: dev-0\n",
            "2021-11-24 08:24:22,092 [INFO ] [absl        ]: tokens: [CLS] flu shot , got . # va ##cci ##nated # flush ##ot # disease ##pre ##vent ##ion # vaccines ##work # flu ##zone # target . . . twitter ##ur ##l [SEP]\n",
            "2021-11-24 08:24:22,093 [INFO ] [absl        ]: input_ids: 101 19857 2915 1010 2288 1012 1001 12436 14693 23854 1001 13862 4140 1001 4295 28139 15338 3258 1001 28896 6198 1001 19857 15975 1001 4539 1012 1012 1012 10474 3126 2140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,093 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,093 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,093 [INFO ] [absl        ]: label: 1 (id = 2)\n",
            "2021-11-24 08:24:22,093 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 08:24:22,093 [INFO ] [absl        ]: guid: dev-1\n",
            "2021-11-24 08:24:22,094 [INFO ] [absl        ]: tokens: [CLS] idiots . twitter ##user : texas me ##as ##les outbreak traced to anti - vaccine mega ##church . twitter ##ur ##l [SEP]\n",
            "2021-11-24 08:24:22,094 [INFO ] [absl        ]: input_ids: 101 28781 1012 10474 20330 1024 3146 2033 3022 4244 8293 9551 2000 3424 1011 17404 13164 22743 1012 10474 3126 2140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,094 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,094 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,094 [INFO ] [absl        ]: label: 1 (id = 2)\n",
            "2021-11-24 08:24:22,094 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 08:24:22,094 [INFO ] [absl        ]: guid: dev-2\n",
            "2021-11-24 08:24:22,094 [INFO ] [absl        ]: tokens: [CLS] at the vet my baby is getting a va ##cci ##nation aw ##w ##w ##w ##w poor baby : - ( [SEP]\n",
            "2021-11-24 08:24:22,094 [INFO ] [absl        ]: input_ids: 101 2012 1996 29525 2026 3336 2003 2893 1037 12436 14693 9323 22091 2860 2860 2860 2860 3532 3336 1024 1011 1006 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,094 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,095 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,095 [INFO ] [absl        ]: label: 1 (id = 2)\n",
            "2021-11-24 08:24:22,095 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 08:24:22,095 [INFO ] [absl        ]: guid: dev-3\n",
            "2021-11-24 08:24:22,095 [INFO ] [absl        ]: tokens: [CLS] \" today in journalism my professor said friend ' s daughter got va ##cci ##nated , took a nap , woke , \" \" was never the same ; \" \" later diagnosed with autism \" [SEP]\n",
            "2021-11-24 08:24:22,095 [INFO ] [absl        ]: input_ids: 101 1000 2651 1999 8083 2026 2934 2056 2767 1005 1055 2684 2288 12436 14693 23854 1010 2165 1037 18996 1010 8271 1010 1000 1000 2001 2196 1996 2168 1025 1000 1000 2101 11441 2007 19465 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,096 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,096 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,096 [INFO ] [absl        ]: label: -1 (id = 0)\n",
            "2021-11-24 08:24:22,096 [INFO ] [absl        ]: *** Example ***\n",
            "2021-11-24 08:24:22,096 [INFO ] [absl        ]: guid: dev-4\n",
            "2021-11-24 08:24:22,096 [INFO ] [absl        ]: tokens: [CLS] twitter ##user and children getting va ##cci ##nated still risk side effects and death . [SEP]\n",
            "2021-11-24 08:24:22,096 [INFO ] [absl        ]: input_ids: 101 10474 20330 1998 2336 2893 12436 14693 23854 2145 3891 2217 3896 1998 2331 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,096 [INFO ] [absl        ]: input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,096 [INFO ] [absl        ]: segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "2021-11-24 08:24:22,097 [INFO ] [absl        ]: label: -1 (id = 0)\n",
            "2021-11-24 08:24:22,384 [INFO ] [__main__    ]: Sucessfully wrote tfrecord files to ../data/finetune/run_2021-11-24_08-24-14_193220_test_run/crowdbreaks/tfrecords\n",
            "2021-11-24 08:24:22,385 [INFO ] [__main__    ]: Saving config to ../data/finetune/run_2021-11-24_08-24-14_193220_test_run/create_finetune_config.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aBbV0eyd0dNx",
        "outputId": "dd376d2c-d659-49b2-a9e3-783cf3359671"
      },
      "source": [
        "# change run_finetune.py and set args tpu_ip require=False nad use_tpu=False\n",
        "# changed line 152 and 153 and removed bucket name\n",
        "# change project name defalt to ct-bert in args\n",
        "\n",
        "%%shell\n",
        "cd drive/MyDrive/ct-bert\n",
        "RUN_PREFIX=testrun                                  # Name your run\n",
        "#BUCKET_NAME=ct-bert                              # Fill in your buckets name here (without the gs:// prefix)\n",
        "#TPU_IP=XX.XX.XXX.X                                  # Fill in your TPUs IP here\n",
        "FINETUNE_DATASET=crowdbreaks                 # Your dataset name\n",
        "FINETUNE_DATA=run_2021-11-24_08-04-37_868161_test_run                       # Fill in dataset run name (e.g. run_2020-05-19_14-14-53_517063_test_run)\n",
        "MODEL_CLASS=covid-twitter-bert\n",
        "TRAIN_BATCH_SIZE=8\n",
        "EVAL_BATCH_SIZE=8\n",
        "LR=2e-5\n",
        "NUM_EPOCHS=1\n",
        "\n",
        "python run_finetune.py --run_prefix $RUN_PREFIX --model_class $MODEL_CLASS --finetune_data ${FINETUNE_DATA}/${FINETUNE_DATASET} --train_batch_size $TRAIN_BATCH_SIZE --eval_batch_size $EVAL_BATCH_SIZE --num_epochs $NUM_EPOCHS --learning_rate $LR "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-24 09:53:13.660681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-11-24 09:53:13.673556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-24 09:53:13.674156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2021-11-24 09:53:13.674414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-11-24 09:53:13.675997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-11-24 09:53:13.677259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-11-24 09:53:13.677570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-11-24 09:53:13.679304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-11-24 09:53:13.680168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-11-24 09:53:13.683656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-11-24 09:53:13.683761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-24 09:53:13.684400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-24 09:53:13.684928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
            "2021-11-24 09:53:13.685230: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-11-24 09:53:13.690034: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199995000 Hz\n",
            "2021-11-24 09:53:13.690261: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ab3dd74840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-11-24 09:53:13.690289: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-11-24 09:53:13.901236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-24 09:53:13.902064: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ab3dd74d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-11-24 09:53:13.902105: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2021-11-24 09:53:13.902289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-24 09:53:13.902842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2021-11-24 09:53:13.902915: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-11-24 09:53:13.902967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-11-24 09:53:13.902989: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-11-24 09:53:13.903011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-11-24 09:53:13.903035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-11-24 09:53:13.903055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-11-24 09:53:13.903075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-11-24 09:53:13.903151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-24 09:53:13.903723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-24 09:53:13.904258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
            "2021-11-24 09:53:13.904322: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-11-24 09:53:13.905483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-11-24 09:53:13.905511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
            "2021-11-24 09:53:13.905527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
            "2021-11-24 09:53:13.905633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-24 09:53:13.906202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-24 09:53:13.906720: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-11-24 09:53:13.906758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15064 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2021-11-24 09:53:13,908 [INFO ] [tensorflow  ]: Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "2021-11-24 09:53:13,909 [INFO ] [__main__    ]: *** Starting run run_2021-11-24_09-53-13_909679_testrun ***\n",
            "2021-11-24 09:53:13,914 [INFO ] [__main__    ]: Loaded training data meta.json file: {'task_type': 'bert_classification', 'processor_type': 'text-classification', 'num_labels': 3, 'train_data_size': 2041, 'max_seq_length': 96, 'eval_data_size': 680}\n",
            "2021-11-24 09:53:13,928 [INFO ] [__main__    ]: Finetuning on datset run_2021-11-24_08-04-37_868161_test_run/crowdbreaks using default pretrained model covid-twitter-bert\n",
            "2021-11-24 09:53:13,929 [INFO ] [__main__    ]: Running 1 epochs with 255 steps per epoch\n",
            "2021-11-24 09:53:13,929 [INFO ] [__main__    ]: Using warmup proportion of 0.1, resulting in 25 warmup steps\n",
            "2021-11-24 09:53:13,929 [INFO ] [__main__    ]: Using learning rate: 2e-05, training batch size: 8, num_epochs: 1\n",
            "2021-11-24 09:53:13,937 [INFO ] [absl        ]: Using gs://None/tmp to cache modules.\n",
            "2021-11-24 09:53:13.937705: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "Traceback (most recent call last):\n",
            "  File \"run_finetune.py\", line 393, in <module>\n",
            "    main(args)\n",
            "  File \"run_finetune.py\", line 350, in main\n",
            "    run(args)\n",
            "  File \"run_finetune.py\", line 180, in run\n",
            "    classifier_model, core_model = get_model(args, model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length)\n",
            "  File \"run_finetune.py\", line 73, in get_model\n",
            "    hub_module_trainable=hub_module_trainable)\n",
            "  File \"tensorflow_models/official/nlp/bert/bert_models.py\", line 313, in classifier_model\n",
            "    hub_module_url, trainable=hub_module_trainable)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py\", line 153, in __init__\n",
            "    self._func = load_module(handle, tags, self._load_options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py\", line 449, in load_module\n",
            "    return module_v2.load(handle, tags=tags, options=set_load_options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_hub/module_v2.py\", line 92, in load\n",
            "    module_path = resolve(handle)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_hub/module_v2.py\", line 47, in resolve\n",
            "    return registry.resolver(handle)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_hub/registry.py\", line 51, in __call__\n",
            "    return impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_hub/compressed_module_resolver.py\", line 57, in __call__\n",
            "    module_dir = _module_dir(handle)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_hub/compressed_module_resolver.py\", line 34, in _module_dir\n",
            "    hashlib.sha1(handle.encode(\"utf8\")).hexdigest())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_hub/resolver.py\", line 117, in create_local_module_dir\n",
            "    tf.compat.v1.gfile.MakeDirs(cache_dir)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 427, in recursive_create_dir\n",
            "    recursive_create_dir_v2(dirname)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 442, in recursive_create_dir_v2\n",
            "    _pywrap_file_io.RecursivelyCreateDir(compat.as_bytes(path))\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: Error executing an HTTP request: HTTP response code 400 with body '{\n",
            "  \"error\": {\n",
            "    \"code\": 400,\n",
            "    \"message\": \"Invalid bucket name: 'None'\",\n",
            "    \"errors\": [\n",
            "      {\n",
            "        \"message\": \"Invalid bucket name: 'None'\",\n",
            "        \"domain\": \"global\",\n",
            "        \"reason\": \"invalid\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "'\n",
            "\t when reading metadata of gs://None/tmp\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-6076513a7b92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cd drive/MyDrive/ct-bert\\nRUN_PREFIX=testrun                                  # Name your run\\n#BUCKET_NAME=ct-bert                              # Fill in your buckets name here (without the gs:// prefix)\\n#TPU_IP=XX.XX.XXX.X                                  # Fill in your TPUs IP here\\nFINETUNE_DATASET=crowdbreaks                 # Your dataset name\\nFINETUNE_DATA=run_2021-11-24_08-04-37_868161_test_run                       # Fill in dataset run name (e.g. run_2020-05-19_14-14-53_517063_test_run)\\nMODEL_CLASS=covid-twitter-bert\\nTRAIN_BATCH_SIZE=8\\nEVAL_BATCH_SIZE=8\\nLR=2e-5\\nNUM_EPOCHS=1\\n\\npython run_finetune.py --run_prefix $RUN_PREFIX --model_class $MODEL_CLASS --finetune_data ${FINETUNE_DATA}/${FINETUNE_DATASET} --train_batch_size $TRAIN_BATCH_SIZE --eval_batch_size $EVAL_BATCH_SIZE --num_epochs $NUM_EPOCHS --learning_rate $LR '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 139\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'cd drive/MyDrive/ct-bert\nRUN_PREFIX=testrun                                  # Name your run\n#BUCKET_NAME=ct-bert                              # Fill in your buckets name here (without the gs:// prefix)\n#TPU_IP=XX.XX.XXX.X                                  # Fill in your TPUs IP here\nFINETUNE_DATASET=crowdbreaks                 # Your dataset name\nFINETUNE_DATA=run_2021-11-24_08-04-37_868161_test_run                       # Fill in dataset run name (e.g. run_2020-05-19_14-14-53_517063_test_run)\nMODEL_CLASS=covid-twitter-bert\nTRAIN_BATCH_SIZE=8\nEVAL_BATCH_SIZE=8\nLR=2e-5\nNUM_EPOCHS=1\n\npython run_finetune.py --run_prefix $RUN_PREFIX --model_class $MODEL_CLASS --finetune_data ${FINETUNE_DATA}/${FINETUNE_DATASET} --train_batch_size $TRAIN_BATCH_SIZE --eval_batch_size $EVAL_BATCH_SIZE --num_epochs $NUM_EPOCHS --learning_rate $LR ' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raLl3-cv-3Wa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}