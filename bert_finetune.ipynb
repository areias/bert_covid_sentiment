{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_finetune.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKLSahCmaAFJVBShBGFs3c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/areias/bert_covid_sentiment/blob/main/bert_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc1lcmJX1zZY"
      },
      "source": [
        "## Training\n",
        "\n",
        "from https://github.com/digitalepidemiologylab/covid-twitter-bert/blob/c87912b409659f40018e839c4124be5ae2486713/run_finetune.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3pnT0gk1OkH",
        "outputId": "af8f65b0-ccda-4a0b-dd33-a96376fb68c3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38g5ufj-LXcu",
        "outputId": "0068cec0-1e02-4172-a5ec-2ae3d1cec2d5"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "covid-twitter-bert  drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR5d6VJH1VPD",
        "outputId": "66d17239-6b95-4c14-b6bf-5ea209114cc3"
      },
      "source": [
        "!git clone -b master https://github.com/digitalepidemiologylab/covid-twitter-bert.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'covid-twitter-bert'...\n",
            "remote: Enumerating objects: 1660, done.\u001b[K\n",
            "remote: Counting objects: 100% (222/222), done.\u001b[K\n",
            "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
            "remote: Total 1660 (delta 150), reused 140 (delta 88), pack-reused 1438\u001b[K\n",
            "Receiving objects: 100% (1660/1660), 3.48 MiB | 14.79 MiB/s, done.\n",
            "Resolving deltas: 100% (1050/1050), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FviqvxIN2ADC",
        "outputId": "34125fcb-1a60-4bb5-f441-7ff8493778c7"
      },
      "source": [
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 4.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.12.0)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 29.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.17.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.19.5)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 35.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.13.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.41.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.6.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onLVqWna4Jra",
        "outputId": "c618ac0f-4917-4968-e3f0-333eaf60d0b1"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBa7T2m_2x2Y",
        "outputId": "7ac271b5-72d7-4650-def9-cf6ba3a8d0e5"
      },
      "source": [
        "!pip install  tensorflow_addons==0.11.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons==0.11.2\n",
            "  Downloading tensorflow_addons-0.11.2-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons==0.11.2) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "UUYILnDx2aiD",
        "outputId": "a845e6c6-206c-48d3-c654-713fd0c27de4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN8GlUhz4snY"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/models-93490036e00f37ecbe6693b9ff4ae488bb8e9270')\n",
        "sys.path.append('/content/covid-twitter-bert')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr_erlWn4pRF"
      },
      "source": [
        "from official.nlp.bert import bert_models\n",
        "from official.utils.misc import distribution_utils\n",
        "from official.nlp.bert import configs as bert_configs\n",
        "from official.modeling import performance\n",
        "from official.nlp.bert import input_pipeline\n",
        "from official.utils.misc import keras_utils"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpIFoL2t6Gg9"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "import logging\n",
        "from logging.handlers import RotatingFileHandler\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scrn_g3e7dnt"
      },
      "source": [
        "import tqdm\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from utils.misc import ArgParseDefault, save_to_json, add_bool_arg\n",
        "from utils.finetune_helpers import Metrics\n",
        "import utils.optimizer\n",
        "from config import PRETRAINED_MODELS"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHbR3Xha9SlI"
      },
      "source": [
        "import datetime\n",
        "def get_run_name():\n",
        "    # Use timestamp to generate a unique run name\n",
        "    ts = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S_%f')\n",
        "    run_name = f'run_{ts}'\n",
        "    return run_name\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "emCDNNzY_D4c",
        "outputId": "178e5a9a-be50-4ccc-aab9-4ffd2440ab86"
      },
      "source": [
        "\"\"\"python run_finetune.py \\\n",
        "  --run_prefix $RUN_PREFIX \\\n",
        "  --bucket_name $BUCKET_NAME \\\n",
        "  --tpu_ip $TPU_IP \\\n",
        "  --model_class $MODEL_CLASS \\\n",
        "  --finetune_data ${FINETUNE_DATA}/${FINETUNE_DATASET} \\\n",
        "  --train_batch_size $TRAIN_BATCH_SIZE \\\n",
        "  --eval_batch_size $EVAL_BATCH_SIZE \\\n",
        "  --num_epochs $NUM_EPOCHS \\\n",
        "  --learning_rate $LR\"\"\"\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'python run_finetune.py   --run_prefix $RUN_PREFIX   --bucket_name $BUCKET_NAME   --tpu_ip $TPU_IP   --model_class $MODEL_CLASS   --finetune_data ${FINETUNE_DATA}/${FINETUNE_DATASET}   --train_batch_size $TRAIN_BATCH_SIZE   --eval_batch_size $EVAL_BATCH_SIZE   --num_epochs $NUM_EPOCHS   --learning_rate $LR'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smsaDoCX-_pV"
      },
      "source": [
        "# args\n",
        "\n",
        "RUN_PREFIX='testrun'                                  # Name your run\n",
        "#BUCKET_NAME=                                        # Fill in your buckets name here (without the gs:// prefix)\n",
        "#TPU_IP=XX.XX.XXX.X                                  # Fill in your TPUs IP here\n",
        "FINETUNE_DATASET='crowdbreaks'                      # Your dataset name\n",
        "FINETUNE_DATA= get_run_name()                         # Fill in dataset run name (e.g. run_2020-05-19_14-14-53_517063_test_run)\n",
        "MODEL_CLASS='covid-twitter-bert'\n",
        "TRAIN_BATCH_SIZE=8 #32\n",
        "EVAL_BATCH_SIZE=8\n",
        "LR=2e-5\n",
        "NUM_EPOCHS=1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEkHqvhF7xef",
        "outputId": "a5b882be-cfe9-4ca5-d7f5-8007422196c1"
      },
      "source": [
        "from collections import namedtuple\n",
        "arguments = namedtuple('arguments', ['run_prefix','model_class','finetune_data', \n",
        "                                     'train_batch_size', 'eval_batch_size','learning_rate',\n",
        "                                     'limit_train_steps','limit_eval_steps','num_epochs',\n",
        "                                     'warmup_proportion', 'init_checkpoint','validation_freq',\n",
        "                                     'end_lr','optimizer_type', 'save_model',\n",
        "                                     'early_stopping_epochs','time_history_log_steps'])\n",
        "\n",
        "args = arguments(RUN_PREFIX,MODEL_CLASS,FINETUNE_DATA,\n",
        "                 TRAIN_BATCH_SIZE,EVAL_BATCH_SIZE,LR,\n",
        "                 None,None,1,\n",
        "                 0.1, None,1,\n",
        "                 0,'adamw',True,\n",
        "                1,10)            \n",
        "args "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "arguments(run_prefix='testrun', model_class='covid-twitter-bert', finetune_data='run_2021-11-19_14-00-25_600389', train_batch_size=8, eval_batch_size=8, learning_rate=2e-05, limit_train_steps=None, limit_eval_steps=None, num_epochs=1, warmup_proportion=0.1, init_checkpoint=None, validation_freq=1, end_lr=0, optimizer_type='adamw', save_model=True, early_stopping_epochs=1, time_history_log_steps=10)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoAbELtb8PMn"
      },
      "source": [
        "def set_mixed_precision_policy(args):\n",
        "    \"\"\"Sets mix precision policy.\"\"\"\n",
        "    if args.dtype == 'fp16':\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16', loss_scale=loss_scale)\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "    elif args.dtype == 'bf16':\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "    elif args.dtype == 'fp32':\n",
        "        tf.keras.mixed_precision.experimental.set_policy('float32')\n",
        "    else:\n",
        "        raise ValueError(f'Unknown dtype {args.dtype}')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUseChVU_kpN"
      },
      "source": [
        "#def run(args):\n",
        "\"\"\"Train using the Keras/TF 2.0. Adapted from the tensorflow/models Github\"\"\"\n",
        "# CONFIG\n",
        "run_name = get_run_name()\n",
        "#logger.info(f'*** Starting run {run_name} ***')\n",
        "data_dir = 'drive/MyDrive/vax_bert_data'\n",
        "output_dir = 'drive/MyDrive/vax_bert_output'\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KFlWFt5K7BY"
      },
      "source": [
        "def get_model_config(config_path):\n",
        "    config = bert_configs.BertConfig.from_json_file(config_path)\n",
        "    return config"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Aisf7mHyLHLc",
        "outputId": "57b89725-0f9d-495c-d2f2-f67150d9f377"
      },
      "source": [
        "PRETRAINED_MODELS[args.model_class]['config']"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bert_config_covid_twitter_bert.json'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdGZg57FMce3",
        "outputId": "b2e8171e-0c39-4c04-a2e5-430f71395a3e"
      },
      "source": [
        "!ls covid-twitter-bert/configs"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_config_covid_twitter_bert.json  bert_config_large_uncased_wwm.json\n",
            "bert_config_large_uncased.json\t     bert_config_multi_cased.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnDO4WqHKzuw"
      },
      "source": [
        "def get_model_config_path(args):\n",
        "    try:\n",
        "        config_path = PRETRAINED_MODELS[args.model_class]['config']\n",
        "    except KeyError:\n",
        "        raise ValueError(f'Could not find a pretrained model matching the model class {args.model_class}')\n",
        "    return os.path.join('covid-twitter-bert/configs/', config_path)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1DyATGDKvn2"
      },
      "source": [
        "# Get configs\n",
        "pretrained_model_config_path = get_model_config_path(args)\n",
        "model_config = get_model_config(pretrained_model_config_path)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p7UwYyzDNU6z",
        "outputId": "4f137be0-09af-46a5-9f2a-dc00eed0845b"
      },
      "source": [
        "pretrained_model_config_path"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'covid-twitter-bert/configs/bert_config_covid_twitter_bert.json'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNQn3y3UNS8I",
        "outputId": "c766f487-978e-4998-9e8d-4cdc0916338d"
      },
      "source": [
        "model_config"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<official.nlp.bert.configs.BertConfig at 0x7fb9dae73810>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uNzLc6CNulB"
      },
      "source": [
        "def get_input_meta_data(data_dir):\n",
        "    with tf.io.gfile.GFile('drive/MyDrive/vax_bert_data/meta.json', 'rb') as reader:\n",
        "        input_meta_data = json.loads(reader.read().decode('utf-8'))\n",
        "    return input_meta_data\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb57DvopTQza"
      },
      "source": [
        "def get_label_mapping(data_dir):\n",
        "    with tf.io.gfile.GFile('drive/MyDrive/vax_bert_data/label_mapping.json', 'rb') as reader:\n",
        "        label_mapping = json.loads(reader.read().decode('utf-8'))\n",
        "    label_mapping = dict(zip(range(len(label_mapping)), label_mapping))\n",
        "    return label_mapping"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxHRsRtYN1S8"
      },
      "source": [
        "# Meta data/label mapping\n",
        "input_meta_data = get_input_meta_data(data_dir)\n",
        "label_mapping = get_label_mapping(data_dir)\n",
        "#logger.info(f'Loaded training data meta.json file: {input_meta_data}')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIakRXGsTpm1"
      },
      "source": [
        "# Calculate steps, warmup steps and eval steps\n",
        "train_data_size = input_meta_data['train_data_size']\n",
        "num_labels = input_meta_data['num_labels']\n",
        "max_seq_length = input_meta_data['max_seq_length']\n",
        "if args.limit_train_steps is None:\n",
        "    steps_per_epoch = int(train_data_size / args.train_batch_size)\n",
        "else:\n",
        "    steps_per_epoch = args.limit_train_steps\n",
        "warmup_steps = int(args.num_epochs * train_data_size * args.warmup_proportion/ args.train_batch_size)\n",
        "if args.limit_eval_steps is None:\n",
        "    eval_steps = int(math.ceil(input_meta_data['eval_data_size'] / args.eval_batch_size))\n",
        "else:\n",
        "    eval_steps = args.limit_eval_steps"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqh7nXarUspT"
      },
      "source": [
        "def get_model(args, model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length, is_hub_module=False):\n",
        "    # Get classifier and core model (used to initialize from checkpoint)\n",
        "    if args.init_checkpoint is None and PRETRAINED_MODELS[args.model_class]['is_tfhub_model']:\n",
        "        # load pretrained model from TF-hub\n",
        "        hub_module_url = f\"https://tfhub.dev/{PRETRAINED_MODELS[args.model_class]['hub_url']}\"\n",
        "        hub_module_trainable = True\n",
        "    else:\n",
        "        hub_module_url = None\n",
        "        hub_module_trainable = False\n",
        "    classifier_model, core_model = bert_models.classifier_model(\n",
        "            model_config,\n",
        "            num_labels,\n",
        "            max_seq_length,\n",
        "            hub_module_url=hub_module_url,\n",
        "            hub_module_trainable=hub_module_trainable)\n",
        "    # Optimizer\n",
        "    optimizer = utils.optimizer.create_optimizer(\n",
        "            args.learning_rate,\n",
        "            steps_per_epoch * args.num_epochs,\n",
        "            warmup_steps,\n",
        "            args.end_lr,\n",
        "            args.optimizer_type)\n",
        "    classifier_model.optimizer = configure_optimizer(\n",
        "            optimizer,\n",
        "            use_float16=False,\n",
        "            use_graph_rewrite=False)\n",
        "    return classifier_model, core_model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7ubHQoLVsrv"
      },
      "source": [
        "def configure_optimizer(optimizer, use_float16=False, use_graph_rewrite=False, loss_scale='dynamic'):\n",
        "    \"\"\"Configures optimizer object with performance options.\"\"\"\n",
        "    if use_float16:\n",
        "        # Wraps optimizer with a LossScaleOptimizer. This is done automatically in compile() with the\n",
        "        # \"mixed_float16\" policy, but since we do not call compile(), we must wrap the optimizer manually.\n",
        "        optimizer = (tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, loss_scale=loss_scale))\n",
        "    if use_graph_rewrite:\n",
        "        # Note: the model dtype must be 'float32', which will ensure\n",
        "        # tf.ckeras.mixed_precision and tf.train.experimental.enable_mixed_precision_graph_rewrite do not double up.\n",
        "        optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(optimizer)\n",
        "    return optimizer"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLLj9CAfV4B_"
      },
      "source": [
        "def get_loss_fn(num_classes):\n",
        "    \"\"\"Gets the classification loss function.\"\"\"\n",
        "    def classification_loss_fn(labels, logits):\n",
        "        \"\"\"Classification loss.\"\"\"\n",
        "        labels = tf.squeeze(labels)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        one_hot_labels = tf.one_hot(tf.cast(labels, dtype=tf.int32), depth=num_classes, dtype=tf.float32)\n",
        "        per_example_loss = -tf.reduce_sum(tf.cast(one_hot_labels, dtype=tf.float32) * log_probs, axis=-1)\n",
        "        return tf.reduce_mean(per_example_loss)\n",
        "    return classification_loss_fn"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IpUfL8eUaTx"
      },
      "source": [
        "# Get model\n",
        "classifier_model, core_model = get_model(args, model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length)\n",
        "optimizer = classifier_model.optimizer\n",
        "loss_fn = get_loss_fn(num_labels)\n",
        "#try:\n",
        "#    if ',' in args.validation_freq:\n",
        "#        validation_freq = args.validation_freq.split(',')\n",
        "#        validation_freq = [int(v) for v in validation_freq]\n",
        "#    else:\n",
        "validation_freq = int(args.validation_freq)\n",
        "#except:\n",
        "#    raise ValueError(f'Invalid argument for validation_freq!')\n",
        "#logger.info(f'Using a validation frequency of {validation_freq}')\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrbklHJ-WtmZ",
        "outputId": "d629550b-1d7b-404a-93e6-b0f353828b1c"
      },
      "source": [
        "classifier_model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1024)         0           keras_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 3)            3075        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 335,144,964\n",
            "Trainable params: 335,144,963\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btcpefmyWazI",
        "outputId": "01c80d17-3786-43ed-fb20-46c8f100cd1c"
      },
      "source": [
        "classifier_model.get_config()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_layers': {'input_mask': ['input_mask', 0, 0],\n",
              "  'input_type_ids': ['input_type_ids', 0, 0],\n",
              "  'input_word_ids': ['input_word_ids', 0, 0]},\n",
              " 'layers': [{'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 96),\n",
              "    'dtype': 'int32',\n",
              "    'name': 'input_word_ids',\n",
              "    'ragged': False,\n",
              "    'sparse': False},\n",
              "   'inbound_nodes': [],\n",
              "   'name': 'input_word_ids'},\n",
              "  {'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 96),\n",
              "    'dtype': 'int32',\n",
              "    'name': 'input_mask',\n",
              "    'ragged': False,\n",
              "    'sparse': False},\n",
              "   'inbound_nodes': [],\n",
              "   'name': 'input_mask'},\n",
              "  {'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 96),\n",
              "    'dtype': 'int32',\n",
              "    'name': 'input_type_ids',\n",
              "    'ragged': False,\n",
              "    'sparse': False},\n",
              "   'inbound_nodes': [],\n",
              "   'name': 'input_type_ids'},\n",
              "  {'class_name': 'KerasLayer',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'handle': 'https://tfhub.dev/digitalepidemiologylab/covid-twitter-bert/1',\n",
              "    'name': 'keras_layer',\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['input_word_ids', 0, 0, {}],\n",
              "     ['input_mask', 0, 0, {}],\n",
              "     ['input_type_ids', 0, 0, {}]]],\n",
              "   'name': 'keras_layer'},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.1,\n",
              "    'seed': None,\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['keras_layer', 0, 0, {}]]],\n",
              "   'name': 'dropout'},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'linear',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'TruncatedNormal',\n",
              "     'config': {'mean': 0.0, 'seed': None, 'stddev': 0.02}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'output',\n",
              "    'trainable': True,\n",
              "    'units': 3,\n",
              "    'use_bias': True},\n",
              "   'inbound_nodes': [[['dropout', 0, 0, {}]]],\n",
              "   'name': 'output'}],\n",
              " 'name': 'model',\n",
              " 'output_layers': [['output', 0, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmQ_fvsOXKaY"
      },
      "source": [
        "def get_metrics():\n",
        "    return [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sbmcj30W_hD"
      },
      "source": [
        "# Run keras compile\n",
        "#logger.info(f'Compiling keras model...')\n",
        "classifier_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=get_metrics())\n",
        "#logger.info(f'... done')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KplnImHSXPU5"
      },
      "source": [
        "# Create all custom callbacks\n",
        "summary_dir = os.path.join(output_dir, 'summaries')\n",
        "summary_callback = tf.keras.callbacks.TensorBoard(summary_dir, profile_batch=0)\n",
        "time_history_callback = keras_utils.TimeHistory(\n",
        "    batch_size=args.train_batch_size,\n",
        "    log_steps=args.time_history_log_steps,\n",
        "    logdir=summary_dir)\n",
        "custom_callbacks = [summary_callback, time_history_callback]\n",
        "if args.save_model:\n",
        "    #logger.info('Using save_model option...')\n",
        "    checkpoint_path = os.path.join(output_dir, 'checkpoint')\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
        "    custom_callbacks.append(checkpoint_callback)\n",
        "if args.early_stopping_epochs > 0:\n",
        "    #logger.info(f'Using early stopping of after {args.early_stopping_epochs} epochs of val_loss not decreasing')\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=args.early_stopping_epochs, monitor='val_loss')\n",
        "    custom_callbacks.append(early_stopping_callback)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoCfM5AVYqxL"
      },
      "source": [
        "def get_dataset_fn(input_file_pattern, max_seq_length, global_batch_size, is_training=True):\n",
        "  \"\"\"Gets a closure to create a dataset.\"\"\"\n",
        "  def _dataset_fn(ctx=None):\n",
        "    \"\"\"Returns tf.data.Dataset for distributed BERT pretraining.\"\"\"\n",
        "    batch_size = ctx.get_per_replica_batch_size(\n",
        "        global_batch_size) if ctx else global_batch_size\n",
        "    dataset = input_pipeline.create_classifier_dataset(\n",
        "        input_file_pattern,\n",
        "        max_seq_length,\n",
        "        batch_size,\n",
        "        is_training=is_training,\n",
        "        input_pipeline_context=ctx)\n",
        "    return dataset\n",
        "\n",
        "  return _dataset_fn"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdiP41HIYi0d"
      },
      "source": [
        "# Generate dataset_fn\n",
        "train_input_fn = get_dataset_fn(\n",
        "    'drive/MyDrive/vax_bert_data/train.tfrecords',\n",
        "    max_seq_length,\n",
        "    args.train_batch_size,\n",
        "    is_training=True)\n",
        "eval_input_fn = get_dataset_fn(\n",
        "    'drive/MyDrive/vax_bert_data/dev.tfrecords',\n",
        "    max_seq_length,\n",
        "    args.eval_batch_size,\n",
        "    is_training=False)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dk8jrhOY0_x"
      },
      "source": [
        "# Add mertrics callback to calculate performance metrics at the end of epoch\n",
        "performance_metrics_callback = Metrics(\n",
        "        eval_input_fn,\n",
        "        label_mapping,\n",
        "        os.path.join(summary_dir, 'metrics'),\n",
        "        eval_steps,\n",
        "        args.eval_batch_size,\n",
        "        validation_freq)\n",
        "custom_callbacks.append(performance_metrics_callback)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klX424ubY-fw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f15b7d8-d81c-4f01-f4da-92c2b70c7865"
      },
      "source": [
        "# Run keras fit\n",
        "time_start = time.time()\n",
        "#logger.info('Run training...')\n",
        "history = classifier_model.fit(\n",
        "    x=train_input_fn(),\n",
        "    validation_data=eval_input_fn(),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=args.num_epochs,\n",
        "    validation_steps=eval_steps,\n",
        "    validation_freq=validation_freq,\n",
        "    callbacks=custom_callbacks,\n",
        "    verbose=1)\n",
        "time_end = time.time()\n",
        "training_time_min = (time_end-time_start)/60\n",
        "#logger.info(f'Finished training after {training_time_min:.1f} min')\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255/255 [==============================] - ETA: 0s - loss: 0.7844 - accuracy: 0.6475\n",
            "Epoch 00001: saving model to drive/MyDrive/vax_bert_output/checkpoint\n",
            "255/255 [==============================] - 415s 2s/step - loss: 0.7844 - accuracy: 0.6475 - val_loss: 0.6171 - val_accuracy: 0.7750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UofuqpqPijJv"
      },
      "source": [
        "# Write training log\n",
        "all_scores = performance_metrics_callback.scores\n",
        "all_predictions = performance_metrics_callback.predictions\n",
        "if len(all_scores) > 0:\n",
        "    final_scores = all_scores[-1]\n",
        "    #logger.info(f'Final eval scores: {final_scores}')\n",
        "else:\n",
        "    final_scores = {}\n",
        "full_history = history.history\n",
        "if len(full_history) > 0:\n",
        "    final_val_loss = full_history['val_loss'][-1]\n",
        "    final_loss = full_history['loss'][-1]\n",
        "    #logger.info(f'Final training loss: {final_loss:.2f}, Final validation loss: {final_val_loss:.2f}')\n",
        "else:\n",
        "    final_val_loss = None\n",
        "    final_loss = None\n",
        "data = {\n",
        "        'created_at': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'run_name': run_name,\n",
        "        'final_loss': final_loss,\n",
        "        'final_val_loss': final_val_loss,\n",
        "        'max_seq_length': max_seq_length,\n",
        "        'num_train_steps': steps_per_epoch * args.num_epochs,\n",
        "        'eval_steps': eval_steps,\n",
        "        'steps_per_epoch': steps_per_epoch,\n",
        "        'training_time_min': training_time_min,\n",
        "        'data_dir': data_dir,\n",
        "        'output_dir': output_dir,\n",
        "        'all_scores': all_scores,\n",
        "        'all_predictions': all_predictions,\n",
        "        'num_labels': num_labels,\n",
        "        'label_mapping': label_mapping,\n",
        "        **full_history,\n",
        "        **final_scores,\n",
        "        'args':args}"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdzEJlDYjB-c"
      },
      "source": [
        "# Write run_log\n",
        "f_path_training_log = os.path.join(output_dir, 'run_logs.json')\n",
        "#logger.info(f'Writing training log to {f_path_training_log}...')\n",
        "save_to_json(data, f_path_training_log)\n",
        "# Write bert config\n",
        "model_config.id2label = label_mapping\n",
        "model_config.label2id = {v:k for k, v in label_mapping.items()}\n",
        "model_config.max_seq_length = max_seq_length\n",
        "model_config.num_labels = num_labels\n",
        "f_path_bert_config = os.path.join(output_dir, 'bert_config.json')\n",
        "#logger.info(f'Writing BERT config to {f_path_bert_config}...')\n",
        "save_to_json(model_config.to_dict(), f_path_bert_config)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wweeqBPN7eRW"
      },
      "source": [
        "#def main(args):\n",
        "    # Set TF Hub caching to bucket\n",
        "    #os.environ['TFHUB_CACHE_DIR'] = os.path.join(f'gs://{args.bucket_name}/tmp')\n",
        "    # Get distribution strategy\n",
        "    #if args.use_tpu:\n",
        "    #    if args.tpu_ip:\n",
        "    #        logger.info(f'Intializing TPU on address {args.tpu_ip}...')\n",
        "    #        tpu_address = f'grpc://{args.tpu_ip}:8470'\n",
        "    #        strategy = distribution_utils.get_distribution_strategy(distribution_strategy='tpu', tpu_address=tpu_address)\n",
        "    #    elif args.tpu_name:\n",
        "    #        logger.info(f'Intializing TPU with name {args.tpu_name}...')\n",
        "    #        cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args.tpu_name)\n",
        "    #        tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "    #        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "    #        strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n",
        "    #    else:\n",
        "    #       raise ValueError(f'You need to either specify a tpu_ip or a tpu_name in order to use a TPU.')\n",
        "    #else:\n",
        "    #    strategy = distribution_utils.get_distribution_strategy(distribution_strategy='mirrored', num_gpus=args.num_gpus)\n",
        "# set mixed precision\n",
        "set_mixed_precision_policy(args)\n",
        "# Run training\n",
        "for repeat in range(args.repeats):\n",
        "    with strategy.scope():\n",
        "        run(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saG4Www4bmUF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}