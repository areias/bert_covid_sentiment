{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_finetune.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/areias/bert_covid_sentiment/blob/main/bert_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc1lcmJX1zZY"
      },
      "source": [
        "## Training\n",
        "\n",
        "from https://github.com/digitalepidemiologylab/covid-twitter-bert/blob/c87912b409659f40018e839c4124be5ae2486713/run_finetune.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3pnT0gk1OkH",
        "outputId": "3cb814ad-8c9f-49e1-aed7-33bb37a1c50f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38g5ufj-LXcu",
        "outputId": "cfe96c66-e4ec-4d2b-951b-f184b30d2980"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR5d6VJH1VPD"
      },
      "source": [
        "#!git clone -b master https://github.com/digitalepidemiologylab/covid-twitter-bert.git"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FviqvxIN2ADC",
        "outputId": "23e2002a-a954-4c09-f9d5-5244dc0256e7"
      },
      "source": [
        "!pip install -r drive/MyDrive/covid-twitter-bert/requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 4.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 3)) (0.12.0)\n",
            "Collecting tensorflow_addons==0.11.2\n",
            "  Downloading tensorflow_addons-0.11.2-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 5)) (4.62.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 6)) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (1.18.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (2.2.4)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 90.9 MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 86.2 MB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 89.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.15.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 89.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.42.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.2.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.4.1)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 45.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 69.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons==0.11.2->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 4)) (2.7.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.3.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 6)) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (0.4.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (1.26.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (1.53.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 8)) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 9)) (1.0.5)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 12)) (4.1.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 12)) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 12)) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client->-r drive/MyDrive/covid-twitter-bert/requirements.txt (line 12)) (0.17.4)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=80cd44da32e0249c9978a75630b6cc8d6615f083603dcd01d5308e2f564fce10\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, h5py, google-api-python-client, gast, unidecode, tensorflow-addons, tensorflow, sentencepiece, emoji, cloud-tpu-client\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.290 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 emoji-1.6.1 gast-0.3.3 google-api-python-client-1.8.0 h5py-2.10.0 sentencepiece-0.1.96 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-addons-0.11.2 tensorflow-estimator-2.2.0 unidecode-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onLVqWna4Jra",
        "outputId": "788a1b23-80f2-4bda-e363-bdf9dfdb9d7f"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN8GlUhz4snY"
      },
      "source": [
        "import sys\n",
        "sys.path.append('drive/MyDrive/covid-twitter-bert')\n",
        "sys.path.append('drive/MyDrive/covid-twitter-bert/tensorflow_models')\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr_erlWn4pRF"
      },
      "source": [
        "from official.nlp.bert import bert_models\n",
        "from official.utils.misc import distribution_utils\n",
        "from official.nlp.bert import configs as bert_configs\n",
        "from official.modeling import performance\n",
        "from official.nlp.bert import input_pipeline\n",
        "from official.utils.misc import keras_utils"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpIFoL2t6Gg9"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "import logging\n",
        "from logging.handlers import RotatingFileHandler\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scrn_g3e7dnt"
      },
      "source": [
        "import tqdm\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from utils.misc import ArgParseDefault, save_to_json, add_bool_arg\n",
        "from utils.finetune_helpers import Metrics\n",
        "import utils.optimizer\n",
        "from config import PRETRAINED_MODELS"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGbs7_x0jiLL",
        "outputId": "4e522fca-88ff-4471-a50a-f4befea37bbd"
      },
      "source": [
        "#change config.py MODELS to point to covid-twitter-2\n",
        "import importlib\n",
        "importlib.reload(__import__('config')); from config import PRETRAINED_MODELS\n",
        "PRETRAINED_MODELS"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert_large_uncased': {'bucket_location': 'pretrained_models/bert/keras_bert/uncased_L-24_H-1024_A-16',\n",
              "  'config': 'bert_config_large_uncased.json',\n",
              "  'do_whole_word_masking': False,\n",
              "  'hub_url': 'tensorflow/bert_en_uncased_L-24_H-1024_A-16/2',\n",
              "  'is_tfhub_model': True,\n",
              "  'lower_case': True,\n",
              "  'vocab_file': 'bert-large-uncased-vocab.txt'},\n",
              " 'bert_large_uncased_wwm': {'bucket_location': 'pretrained_models/bert/keras_bert/wwm_uncased_L-24_H-1024_A-16',\n",
              "  'config': 'bert_config_large_uncased_wwm.json',\n",
              "  'do_whole_word_masking': True,\n",
              "  'hub_url': 'tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/2',\n",
              "  'is_tfhub_model': True,\n",
              "  'lower_case': True,\n",
              "  'vocab_file': 'bert-large-uncased-whole-word-masking-vocab.txt'},\n",
              " 'bert_multi_cased': {'bucket_location': 'pretrained_models/bert/keras_bert/multi_cased_L-12_H-768_A-12',\n",
              "  'config': 'bert_config_multi_cased.json',\n",
              "  'do_whole_word_masking': False,\n",
              "  'hub_url': 'tensorflow/bert_multi_cased_L-12_H-768_A-12/2',\n",
              "  'is_tfhub_model': True,\n",
              "  'lower_case': False,\n",
              "  'vocab_file': 'bert-multi-cased-vocab.txt'},\n",
              " 'covid-twitter-bert': {'config': 'bert_config_covid_twitter_bert.json',\n",
              "  'do_whole_word_masking': True,\n",
              "  'hub_url': 'digitalepidemiologylab/covid-twitter-bert/2',\n",
              "  'is_tfhub_model': True,\n",
              "  'lower_case': True,\n",
              "  'vocab_file': 'bert-large-uncased-whole-word-masking-vocab.txt'}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHbR3Xha9SlI"
      },
      "source": [
        "import datetime\n",
        "def get_run_name():\n",
        "    # Use timestamp to generate a unique run name\n",
        "    ts = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S_%f')\n",
        "    run_name = f'run_{ts}'\n",
        "    return run_name\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "emCDNNzY_D4c",
        "outputId": "7b91b4d3-87d4-4a75-e945-2f78e040639d"
      },
      "source": [
        "\"\"\"python run_finetune.py \\\n",
        "  --run_prefix $RUN_PREFIX \\\n",
        "  --bucket_name $BUCKET_NAME \\\n",
        "  --tpu_ip $TPU_IP \\\n",
        "  --model_class $MODEL_CLASS \\\n",
        "  --finetune_data ${FINETUNE_DATA}/${FINETUNE_DATASET} \\\n",
        "  --train_batch_size $TRAIN_BATCH_SIZE \\\n",
        "  --eval_batch_size $EVAL_BATCH_SIZE \\\n",
        "  --num_epochs $NUM_EPOCHS \\\n",
        "  --learning_rate $LR\"\"\"\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'python run_finetune.py   --run_prefix $RUN_PREFIX   --bucket_name $BUCKET_NAME   --tpu_ip $TPU_IP   --model_class $MODEL_CLASS   --finetune_data ${FINETUNE_DATA}/${FINETUNE_DATASET}   --train_batch_size $TRAIN_BATCH_SIZE   --eval_batch_size $EVAL_BATCH_SIZE   --num_epochs $NUM_EPOCHS   --learning_rate $LR'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smsaDoCX-_pV"
      },
      "source": [
        "# args\n",
        "\n",
        "RUN_PREFIX='testrun'                                  # Name your run\n",
        "#BUCKET_NAME=                                        # Fill in your buckets name here (without the gs:// prefix)\n",
        "#TPU_IP=XX.XX.XXX.X                                  # Fill in your TPUs IP here\n",
        "FINETUNE_DATASET='crowdbreaks'                      # Your dataset name\n",
        "FINETUNE_DATA= 'run_2021-11-24_16-39-54_269137_test_run'                       # Fill in dataset run name (e.g. run_2020-05-19_14-14-53_517063_test_run)\n",
        "MODEL_CLASS='covid-twitter-bert'\n",
        "TRAIN_BATCH_SIZE=16 #32b\n",
        "EVAL_BATCH_SIZE=8\n",
        "LR=2e-5\n",
        "NUM_EPOCHS=2"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEkHqvhF7xef",
        "outputId": "fe1f4caf-c5e1-4a27-bea0-f172c70c463a"
      },
      "source": [
        "from collections import namedtuple\n",
        "arguments = namedtuple('arguments', ['run_prefix','model_class','finetune_data', \n",
        "                                     'train_batch_size', 'eval_batch_size','learning_rate',\n",
        "                                     'limit_train_steps','limit_eval_steps','num_epochs',\n",
        "                                     'warmup_proportion', 'init_checkpoint','validation_freq',\n",
        "                                     'end_lr','optimizer_type', 'save_model',\n",
        "                                     'early_stopping_epochs','time_history_log_steps'])\n",
        "\n",
        "args = arguments(RUN_PREFIX,MODEL_CLASS,FINETUNE_DATA,\n",
        "                 TRAIN_BATCH_SIZE,EVAL_BATCH_SIZE,LR,\n",
        "                 None,None,1,\n",
        "                 0.1, None,1,\n",
        "                 0,'adamw',True,\n",
        "                1,10)            \n",
        "args "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "arguments(run_prefix='testrun', model_class='covid-twitter-bert', finetune_data='run_2021-11-24_16-39-54_269137_test_run', train_batch_size=16, eval_batch_size=8, learning_rate=2e-05, limit_train_steps=None, limit_eval_steps=None, num_epochs=1, warmup_proportion=0.1, init_checkpoint=None, validation_freq=1, end_lr=0, optimizer_type='adamw', save_model=True, early_stopping_epochs=1, time_history_log_steps=10)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoAbELtb8PMn"
      },
      "source": [
        "def set_mixed_precision_policy(args):\n",
        "    \"\"\"Sets mix precision policy.\"\"\"\n",
        "    if args.dtype == 'fp16':\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16', loss_scale=loss_scale)\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "    elif args.dtype == 'bf16':\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "    elif args.dtype == 'fp32':\n",
        "        tf.keras.mixed_precision.experimental.set_policy('float32')\n",
        "    else:\n",
        "        raise ValueError(f'Unknown dtype {args.dtype}')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUseChVU_kpN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "eaa40a30-988e-4107-e12b-d9fbc4627706"
      },
      "source": [
        "#def run(args):\n",
        "\"\"\"Train using the Keras/TF 2.0. Adapted from the tensorflow/models Github\"\"\"\n",
        "# CONFIG\n",
        "run_name = get_run_name()\n",
        "#logger.info(f'*** Starting run {run_name} ***')\n",
        "data_dir = 'drive/MyDrive/covid-twitter-bert/data/finetune/'+args.finetune_data+'/crowdbreaks/'\n",
        "output_dir = 'drive/MyDrive/covid-twitter-bert/data/finetune/'+args.finetune_data+'/crowdbreaks/'\n",
        "\n",
        "output_dir"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/covid-twitter-bert/data/finetune/run_2021-11-24_16-39-54_269137_test_run/crowdbreaks/'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KFlWFt5K7BY"
      },
      "source": [
        "def get_model_config(config_path):\n",
        "    config = bert_configs.BertConfig.from_json_file(config_path)\n",
        "    return config"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Aisf7mHyLHLc",
        "outputId": "d3fbb432-10b2-4823-d81e-03733c04bdb9"
      },
      "source": [
        "PRETRAINED_MODELS[args.model_class]['config']"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bert_config_covid_twitter_bert.json'"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnDO4WqHKzuw"
      },
      "source": [
        "def get_model_config_path(args):\n",
        "    try:\n",
        "        config_path = PRETRAINED_MODELS[args.model_class]['config']\n",
        "    except KeyError:\n",
        "        raise ValueError(f'Could not find a pretrained model matching the model class {args.model_class}')\n",
        "    return os.path.join('drive/MyDrive/covid-twitter-bert/configs/', config_path)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1DyATGDKvn2"
      },
      "source": [
        "# Get configs\n",
        "pretrained_model_config_path = get_model_config_path(args)\n",
        "model_config = get_model_config(pretrained_model_config_path)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "p7UwYyzDNU6z",
        "outputId": "606cdcdc-ea0e-4e2a-a7e4-906ca8c36eb2"
      },
      "source": [
        "pretrained_model_config_path"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/covid-twitter-bert/configs/bert_config_covid_twitter_bert.json'"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNQn3y3UNS8I",
        "outputId": "55ce36a3-105b-4f35-8ee7-18f3c654af3c"
      },
      "source": [
        "model_config"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<official.nlp.bert.configs.BertConfig at 0x7f4620cd17d0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uNzLc6CNulB"
      },
      "source": [
        "def get_input_meta_data(data_dir):\n",
        "    with tf.io.gfile.GFile(data_dir+'/meta.json', 'rb') as reader:\n",
        "        input_meta_data = json.loads(reader.read().decode('utf-8'))\n",
        "    return input_meta_data\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb57DvopTQza"
      },
      "source": [
        "def get_label_mapping(data_dir):\n",
        "    with tf.io.gfile.GFile(data_dir+'/label_mapping.json', 'rb') as reader:\n",
        "        label_mapping = json.loads(reader.read().decode('utf-8'))\n",
        "    label_mapping = dict(zip(range(len(label_mapping)), label_mapping))\n",
        "    return label_mapping"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxHRsRtYN1S8"
      },
      "source": [
        "# Meta data/label mapping\n",
        "input_meta_data = get_input_meta_data(data_dir)\n",
        "label_mapping = get_label_mapping(data_dir)\n",
        "#logger.info(f'Loaded training data meta.json file: {input_meta_data}')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfjFRhd2T4Cz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07833549-c8f3-4676-ba7e-373215eca40e"
      },
      "source": [
        "\n",
        "\n",
        "input_meta_data"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_data_size': 680,\n",
              " 'max_seq_length': 96,\n",
              " 'num_labels': 3,\n",
              " 'processor_type': 'text-classification',\n",
              " 'task_type': 'bert_classification',\n",
              " 'train_data_size': 2041}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx-OiPqJT6K7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef1045e-ffd1-4312-e525-ffca455e37cc"
      },
      "source": [
        "label_mapping"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: -1, 1: 0, 2: 1}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIakRXGsTpm1"
      },
      "source": [
        "# Calculate steps, warmup steps and eval steps\n",
        "train_data_size = input_meta_data['train_data_size']\n",
        "num_labels = input_meta_data['num_labels']\n",
        "max_seq_length = input_meta_data['max_seq_length']\n",
        "if args.limit_train_steps is None:\n",
        "    steps_per_epoch = int(train_data_size / args.train_batch_size)\n",
        "else:\n",
        "    steps_per_epoch = args.limit_train_steps\n",
        "warmup_steps = int(args.num_epochs * train_data_size * args.warmup_proportion/ args.train_batch_size)\n",
        "if args.limit_eval_steps is None:\n",
        "    eval_steps = int(math.ceil(input_meta_data['eval_data_size'] / args.eval_batch_size))\n",
        "else:\n",
        "    eval_steps = args.limit_eval_steps"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqh7nXarUspT"
      },
      "source": [
        "def get_model(args, model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length, is_hub_module=False):\n",
        "    # Get classifier and core model (used to initialize from checkpoint)\n",
        "    if args.init_checkpoint is None and PRETRAINED_MODELS[args.model_class]['is_tfhub_model']:\n",
        "        # load pretrained model from TF-hub\n",
        "        hub_module_url = f\"https://tfhub.dev/{PRETRAINED_MODELS[args.model_class]['hub_url']}\"\n",
        "        hub_module_trainable = True\n",
        "    else:\n",
        "        hub_module_url = None\n",
        "        hub_module_trainable = False\n",
        "    classifier_model, core_model = bert_models.classifier_model(\n",
        "            model_config,\n",
        "            num_labels,\n",
        "            max_seq_length,\n",
        "            hub_module_url=hub_module_url,\n",
        "            hub_module_trainable=hub_module_trainable)\n",
        "    # Optimizer\n",
        "    optimizer = utils.optimizer.create_optimizer(\n",
        "            args.learning_rate,\n",
        "            steps_per_epoch * args.num_epochs,\n",
        "            warmup_steps,\n",
        "            args.end_lr,\n",
        "            args.optimizer_type)\n",
        "    classifier_model.optimizer = configure_optimizer(\n",
        "            optimizer,\n",
        "            use_float16=False,\n",
        "            use_graph_rewrite=False)\n",
        "    return classifier_model, core_model"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7ubHQoLVsrv"
      },
      "source": [
        "def configure_optimizer(optimizer, use_float16=False, use_graph_rewrite=False, loss_scale='dynamic'):\n",
        "    \"\"\"Configures optimizer object with performance options.\"\"\"\n",
        "    if use_float16:\n",
        "        # Wraps optimizer with a LossScaleOptimizer. This is done automatically in compile() with the\n",
        "        # \"mixed_float16\" policy, but since we do not call compile(), we must wrap the optimizer manually.\n",
        "        optimizer = (tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, loss_scale=loss_scale))\n",
        "    if use_graph_rewrite:\n",
        "        # Note: the model dtype must be 'float32', which will ensure\n",
        "        # tf.ckeras.mixed_precision and tf.train.experimental.enable_mixed_precision_graph_rewrite do not double up.\n",
        "        optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(optimizer)\n",
        "    return optimizer"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLLj9CAfV4B_"
      },
      "source": [
        "def get_loss_fn(num_classes):\n",
        "    \"\"\"Gets the classification loss function.\"\"\"\n",
        "    def classification_loss_fn(labels, logits):\n",
        "        \"\"\"Classification loss.\"\"\"\n",
        "        labels = tf.squeeze(labels)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        one_hot_labels = tf.one_hot(tf.cast(labels, dtype=tf.int32), depth=num_classes, dtype=tf.float32)\n",
        "        per_example_loss = -tf.reduce_sum(tf.cast(one_hot_labels, dtype=tf.float32) * log_probs, axis=-1)\n",
        "        return tf.reduce_mean(per_example_loss)\n",
        "    return classification_loss_fn"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IpUfL8eUaTx"
      },
      "source": [
        "# Get model\n",
        "classifier_model, core_model = get_model(args, model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length)\n",
        "optimizer = classifier_model.optimizer\n",
        "loss_fn = get_loss_fn(num_labels)\n",
        "#try:\n",
        "#    if ',' in args.validation_freq:\n",
        "#        validation_freq = args.validation_freq.split(',')\n",
        "#        validation_freq = [int(v) for v in validation_freq]\n",
        "#    else:\n",
        "validation_freq = int(args.validation_freq)\n",
        "#except:\n",
        "#    raise ValueError(f'Invalid argument for validation_freq!')\n",
        "#logger.info(f'Using a validation frequency of {validation_freq}')\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrbklHJ-WtmZ",
        "outputId": "d0e1e05e-4694-47ad-e442-ab114876cbd0"
      },
      "source": [
        "classifier_model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1024)         0           keras_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 3)            3075        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 335,144,964\n",
            "Trainable params: 335,144,963\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btcpefmyWazI",
        "outputId": "49273ca3-9875-4d7a-f536-0bdd8eb0433b"
      },
      "source": [
        "classifier_model.get_config()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_layers': {'input_mask': ['input_mask', 0, 0],\n",
              "  'input_type_ids': ['input_type_ids', 0, 0],\n",
              "  'input_word_ids': ['input_word_ids', 0, 0]},\n",
              " 'layers': [{'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 96),\n",
              "    'dtype': 'int32',\n",
              "    'name': 'input_word_ids',\n",
              "    'ragged': False,\n",
              "    'sparse': False},\n",
              "   'inbound_nodes': [],\n",
              "   'name': 'input_word_ids'},\n",
              "  {'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 96),\n",
              "    'dtype': 'int32',\n",
              "    'name': 'input_mask',\n",
              "    'ragged': False,\n",
              "    'sparse': False},\n",
              "   'inbound_nodes': [],\n",
              "   'name': 'input_mask'},\n",
              "  {'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 96),\n",
              "    'dtype': 'int32',\n",
              "    'name': 'input_type_ids',\n",
              "    'ragged': False,\n",
              "    'sparse': False},\n",
              "   'inbound_nodes': [],\n",
              "   'name': 'input_type_ids'},\n",
              "  {'class_name': 'KerasLayer',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'handle': 'https://tfhub.dev/digitalepidemiologylab/covid-twitter-bert/2',\n",
              "    'name': 'keras_layer',\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['input_word_ids', 0, 0, {}],\n",
              "     ['input_mask', 0, 0, {}],\n",
              "     ['input_type_ids', 0, 0, {}]]],\n",
              "   'name': 'keras_layer'},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.1,\n",
              "    'seed': None,\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['keras_layer', 0, 0, {}]]],\n",
              "   'name': 'dropout'},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'linear',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'TruncatedNormal',\n",
              "     'config': {'mean': 0.0, 'seed': None, 'stddev': 0.02}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'output',\n",
              "    'trainable': True,\n",
              "    'units': 3,\n",
              "    'use_bias': True},\n",
              "   'inbound_nodes': [[['dropout', 0, 0, {}]]],\n",
              "   'name': 'output'}],\n",
              " 'name': 'model',\n",
              " 'output_layers': [['output', 0, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJsQ5_-A2mgi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmQ_fvsOXKaY"
      },
      "source": [
        "def get_metrics():\n",
        "    return [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sbmcj30W_hD"
      },
      "source": [
        "# Run keras compile\n",
        "#logger.info(f'Compiling keras model...')\n",
        "classifier_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=get_metrics())\n",
        "#logger.info(f'... done')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KplnImHSXPU5"
      },
      "source": [
        "# Create all custom callbacks\n",
        "summary_dir = os.path.join(output_dir, 'summaries')\n",
        "summary_callback = tf.keras.callbacks.TensorBoard(summary_dir, profile_batch=0)\n",
        "time_history_callback = keras_utils.TimeHistory(\n",
        "    batch_size=args.train_batch_size,\n",
        "    log_steps=args.time_history_log_steps,\n",
        "    logdir=summary_dir)\n",
        "custom_callbacks = [summary_callback, time_history_callback]\n",
        "if args.save_model:\n",
        "    #logger.info('Using save_model option...')\n",
        "    checkpoint_path = os.path.join(output_dir, 'checkpoint')\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
        "    custom_callbacks.append(checkpoint_callback)\n",
        "if args.early_stopping_epochs > 0:\n",
        "    #logger.info(f'Using early stopping of after {args.early_stopping_epochs} epochs of val_loss not decreasing')\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=args.early_stopping_epochs, monitor='val_loss')\n",
        "    custom_callbacks.append(early_stopping_callback)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoCfM5AVYqxL"
      },
      "source": [
        "def get_dataset_fn(input_file_pattern, max_seq_length, global_batch_size, is_training=True):\n",
        "  \"\"\"Gets a closure to create a dataset.\"\"\"\n",
        "  def _dataset_fn(ctx=None):\n",
        "    \"\"\"Returns tf.data.Dataset for distributed BERT pretraining.\"\"\"\n",
        "    batch_size = ctx.get_per_replica_batch_size(\n",
        "        global_batch_size) if ctx else global_batch_size\n",
        "    dataset = input_pipeline.create_classifier_dataset(\n",
        "        input_file_pattern,\n",
        "        max_seq_length,\n",
        "        batch_size,\n",
        "        is_training=is_training,\n",
        "        input_pipeline_context=ctx)\n",
        "    return dataset\n",
        "\n",
        "  return _dataset_fn"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdiP41HIYi0d"
      },
      "source": [
        "# Generate dataset_fn\n",
        "train_input_fn = get_dataset_fn(\n",
        "    data_dir+'/tfrecords/train.tfrecords',\n",
        "    max_seq_length,\n",
        "    args.train_batch_size,\n",
        "    is_training=True)\n",
        "eval_input_fn = get_dataset_fn(\n",
        "    data_dir+'/tfrecords/dev.tfrecords',\n",
        "    max_seq_length,\n",
        "    args.eval_batch_size,\n",
        "    is_training=False)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "EzMuDPdFlAh-",
        "outputId": "11380641-f242-4d23-e8f7-3df441d7e8fa"
      },
      "source": [
        "data_dir"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/covid-twitter-bert/data/finetune/run_2021-11-24_16-39-54_269137_test_run/crowdbreaks/'"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dk8jrhOY0_x"
      },
      "source": [
        "# Add mertrics callback to calculate performance metrics at the end of epoch\n",
        "performance_metrics_callback = Metrics(\n",
        "        eval_input_fn,\n",
        "        label_mapping,\n",
        "        os.path.join(summary_dir, 'metrics'),\n",
        "        eval_steps,\n",
        "        args.eval_batch_size,\n",
        "        validation_freq)\n",
        "custom_callbacks.append(performance_metrics_callback)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klX424ubY-fw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44b8517-d1ce-4df2-cfbd-9dd0916528cb"
      },
      "source": [
        "# Run keras fit\n",
        "time_start = time.time()\n",
        "#logger.info('Run training...')\n",
        "history = classifier_model.fit(\n",
        "    x=train_input_fn(),\n",
        "    validation_data=eval_input_fn(),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=args.num_epochs,\n",
        "    validation_steps=eval_steps,\n",
        "    validation_freq=validation_freq,\n",
        "    callbacks=custom_callbacks,\n",
        "    verbose=1)\n",
        "time_end = time.time()\n",
        "training_time_min = (time_end-time_start)/60\n",
        "#logger.info(f'Finished training after {training_time_min:.1f} min')\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127/127 [==============================] - ETA: 0s - loss: 1.1023 - accuracy: 0.3765\n",
            "Epoch 00001: saving model to drive/MyDrive/covid-twitter-bert/data/finetune/run_2021-11-24_16-39-54_269137_test_run/crowdbreaks/checkpoint\n",
            "127/127 [==============================] - 132s 1s/step - loss: 1.1023 - accuracy: 0.3765 - val_loss: 1.0646 - val_accuracy: 0.4279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UofuqpqPijJv"
      },
      "source": [
        "# Write training log\n",
        "all_scores = performance_metrics_callback.scores\n",
        "all_predictions = performance_metrics_callback.predictions\n",
        "if len(all_scores) > 0:\n",
        "    final_scores = all_scores[-1]\n",
        "    #logger.info(f'Final eval scores: {final_scores}')\n",
        "else:\n",
        "    final_scores = {}\n",
        "full_history = history.history\n",
        "if len(full_history) > 0:\n",
        "    final_val_loss = full_history['val_loss'][-1]\n",
        "    final_loss = full_history['loss'][-1]\n",
        "    #logger.info(f'Final training loss: {final_loss:.2f}, Final validation loss: {final_val_loss:.2f}')\n",
        "else:\n",
        "    final_val_loss = None\n",
        "    final_loss = None\n",
        "data = {\n",
        "        'created_at': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'run_name': run_name,\n",
        "        'final_loss': final_loss,\n",
        "        'final_val_loss': final_val_loss,\n",
        "        'max_seq_length': max_seq_length,\n",
        "        'num_train_steps': steps_per_epoch * args.num_epochs,\n",
        "        'eval_steps': eval_steps,\n",
        "        'steps_per_epoch': steps_per_epoch,\n",
        "        'training_time_min': training_time_min,\n",
        "        'data_dir': data_dir,\n",
        "        'output_dir': output_dir,\n",
        "        'all_scores': all_scores,\n",
        "        'all_predictions': all_predictions,\n",
        "        'num_labels': num_labels,\n",
        "        'label_mapping': label_mapping,\n",
        "        **full_history,\n",
        "        **final_scores,\n",
        "        'args':args}"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdzEJlDYjB-c"
      },
      "source": [
        "# Write run_log\n",
        "f_path_training_log = os.path.join(output_dir, 'run_logs.json')\n",
        "#logger.info(f'Writing training log to {f_path_training_log}...')\n",
        "save_to_json(data, f_path_training_log)\n",
        "# Write bert config\n",
        "model_config.id2label = label_mapping\n",
        "model_config.label2id = {v:k for k, v in label_mapping.items()}\n",
        "model_config.max_seq_length = max_seq_length\n",
        "model_config.num_labels = num_labels\n",
        "f_path_bert_config = os.path.join(output_dir, 'bert_config.json')\n",
        "#logger.info(f'Writing BERT config to {f_path_bert_config}...')\n",
        "save_to_json(model_config.to_dict(), f_path_bert_config)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saG4Www4bmUF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8f36fbb3-7f24-49d7-c13a-647f5fd8dd52"
      },
      "source": [
        "f_path_training_log"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/covid-twitter-bert/data/finetune/run_2021-11-24_16-39-54_269137_test_run/crowdbreaks/run_logs.json'"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NguCFn4V3ZP6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}