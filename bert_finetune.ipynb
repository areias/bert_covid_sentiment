{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_finetune.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/areias/bert_covid_sentiment/blob/main/bert_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc1lcmJX1zZY"
      },
      "source": [
        "## Training\n",
        "\n",
        "from https://github.com/digitalepidemiologylab/covid-twitter-bert/blob/c87912b409659f40018e839c4124be5ae2486713/run_finetune.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3pnT0gk1OkH",
        "outputId": "b2f79fe6-e90c-446a-fd01-93268cf4b681"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38g5ufj-LXcu",
        "outputId": "cb500bb6-0a54-4bac-88a8-12be8717214c"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR5d6VJH1VPD",
        "outputId": "66d17239-6b95-4c14-b6bf-5ea209114cc3"
      },
      "source": [
        "#!git clone -b master https://github.com/digitalepidemiologylab/covid-twitter-bert.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'covid-twitter-bert'...\n",
            "remote: Enumerating objects: 1660, done.\u001b[K\n",
            "remote: Counting objects: 100% (222/222), done.\u001b[K\n",
            "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
            "remote: Total 1660 (delta 150), reused 140 (delta 88), pack-reused 1438\u001b[K\n",
            "Receiving objects: 100% (1660/1660), 3.48 MiB | 14.79 MiB/s, done.\n",
            "Resolving deltas: 100% (1050/1050), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FviqvxIN2ADC",
        "outputId": "aff35756-d660-4e72-de7e-22b7ee45e149"
      },
      "source": [
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 4.1 kB/s \n",
            "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 77.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 61.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 62.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.42.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.13.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.1)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onLVqWna4Jra",
        "outputId": "75f810d6-f978-4ce5-d6f6-e0133be7e7c0"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBa7T2m_2x2Y",
        "outputId": "2c44f1f5-0005-4191-868a-25a028884d1d"
      },
      "source": [
        "!pip install  tensorflow_addons==0.11.2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons==0.11.2\n",
            "  Downloading tensorflow_addons-0.11.2-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 38.2 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 38.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 256 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 286 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 317 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 348 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 409 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 440 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 471 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 501 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 512 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 532 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 542 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 563 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 573 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 593 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 604 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 624 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 634 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 665 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 686 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 696 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 716 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 727 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 747 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 768 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 778 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 788 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 798 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 808 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 819 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 829 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 839 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 849 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 870 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 880 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 890 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 901 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 911 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 921 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 942 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 952 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 972 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 983 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 9.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons==0.11.2) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "UUYILnDx2aiD",
        "outputId": "6a04774d-9c35-45f4-ef67-e62c8728dd20"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 63.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 80.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN8GlUhz4snY"
      },
      "source": [
        "import sys\n",
        "sys.path.append('drive/MyDrive/ct-bert')\n",
        "sys.path.append('drive/MyDrive/ct-bert/tensorflow_models')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr_erlWn4pRF"
      },
      "source": [
        "from official.nlp.bert import bert_models\n",
        "from official.utils.misc import distribution_utils\n",
        "from official.nlp.bert import configs as bert_configs\n",
        "from official.modeling import performance\n",
        "from official.nlp.bert import input_pipeline\n",
        "from official.utils.misc import keras_utils"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpIFoL2t6Gg9"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "import logging\n",
        "from logging.handlers import RotatingFileHandler\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scrn_g3e7dnt"
      },
      "source": [
        "import tqdm\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from utils.misc import ArgParseDefault, save_to_json, add_bool_arg\n",
        "from utils.finetune_helpers import Metrics\n",
        "import utils.optimizer\n",
        "from config import PRETRAINED_MODELS"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHbR3Xha9SlI"
      },
      "source": [
        "import datetime\n",
        "def get_run_name():\n",
        "    # Use timestamp to generate a unique run name\n",
        "    ts = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S_%f')\n",
        "    run_name = f'run_{ts}'\n",
        "    return run_name\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "emCDNNzY_D4c",
        "outputId": "40de54f6-85a9-4364-b2f8-c4138e2d9d75"
      },
      "source": [
        "\"\"\"python run_finetune.py \\\n",
        "  --run_prefix $RUN_PREFIX \\\n",
        "  --bucket_name $BUCKET_NAME \\\n",
        "  --tpu_ip $TPU_IP \\\n",
        "  --model_class $MODEL_CLASS \\\n",
        "  --finetune_data ${FINETUNE_DATA}/${FINETUNE_DATASET} \\\n",
        "  --train_batch_size $TRAIN_BATCH_SIZE \\\n",
        "  --eval_batch_size $EVAL_BATCH_SIZE \\\n",
        "  --num_epochs $NUM_EPOCHS \\\n",
        "  --learning_rate $LR\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'python run_finetune.py   --run_prefix $RUN_PREFIX   --bucket_name $BUCKET_NAME   --tpu_ip $TPU_IP   --model_class $MODEL_CLASS   --finetune_data ${FINETUNE_DATA}/${FINETUNE_DATASET}   --train_batch_size $TRAIN_BATCH_SIZE   --eval_batch_size $EVAL_BATCH_SIZE   --num_epochs $NUM_EPOCHS   --learning_rate $LR'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smsaDoCX-_pV"
      },
      "source": [
        "# args\n",
        "\n",
        "RUN_PREFIX='testrun'                                  # Name your run\n",
        "#BUCKET_NAME=                                        # Fill in your buckets name here (without the gs:// prefix)\n",
        "#TPU_IP=XX.XX.XXX.X                                  # Fill in your TPUs IP here\n",
        "FINETUNE_DATASET='crowdbreaks'                      # Your dataset name\n",
        "FINETUNE_DATA= get_run_name()                         # Fill in dataset run name (e.g. run_2020-05-19_14-14-53_517063_test_run)\n",
        "MODEL_CLASS='bert_large_uncased_wwm'\n",
        "TRAIN_BATCH_SIZE=8 #32\n",
        "EVAL_BATCH_SIZE=8\n",
        "LR=2e-5\n",
        "NUM_EPOCHS=1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEkHqvhF7xef",
        "outputId": "25d472f8-25a6-4b1e-dcc5-ef337a9d7526"
      },
      "source": [
        "from collections import namedtuple\n",
        "arguments = namedtuple('arguments', ['run_prefix','model_class','finetune_data', \n",
        "                                     'train_batch_size', 'eval_batch_size','learning_rate',\n",
        "                                     'limit_train_steps','limit_eval_steps','num_epochs',\n",
        "                                     'warmup_proportion', 'init_checkpoint','validation_freq',\n",
        "                                     'end_lr','optimizer_type', 'save_model',\n",
        "                                     'early_stopping_epochs','time_history_log_steps'])\n",
        "\n",
        "args = arguments(RUN_PREFIX,MODEL_CLASS,FINETUNE_DATA,\n",
        "                 TRAIN_BATCH_SIZE,EVAL_BATCH_SIZE,LR,\n",
        "                 None,None,1,\n",
        "                 0.1, None,1,\n",
        "                 0,'adamw',True,\n",
        "                1,10)            \n",
        "args "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "arguments(run_prefix='testrun', model_class='bert_large_uncased_wwm', finetune_data='run_2021-11-24_14-12-42_144149', train_batch_size=8, eval_batch_size=8, learning_rate=2e-05, limit_train_steps=None, limit_eval_steps=None, num_epochs=1, warmup_proportion=0.1, init_checkpoint=None, validation_freq=1, end_lr=0, optimizer_type='adamw', save_model=True, early_stopping_epochs=1, time_history_log_steps=10)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoAbELtb8PMn"
      },
      "source": [
        "def set_mixed_precision_policy(args):\n",
        "    \"\"\"Sets mix precision policy.\"\"\"\n",
        "    if args.dtype == 'fp16':\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16', loss_scale=loss_scale)\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "    elif args.dtype == 'bf16':\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "    elif args.dtype == 'fp32':\n",
        "        tf.keras.mixed_precision.experimental.set_policy('float32')\n",
        "    else:\n",
        "        raise ValueError(f'Unknown dtype {args.dtype}')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUseChVU_kpN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c67d7994-06f9-4b8c-b55d-0ba25fdaffc8"
      },
      "source": [
        "#def run(args):\n",
        "\"\"\"Train using the Keras/TF 2.0. Adapted from the tensorflow/models Github\"\"\"\n",
        "# CONFIG\n",
        "run_name = 'run_2021-11-24_07-56-06_744272_test_run'\n",
        "#logger.info(f'*** Starting run {run_name} ***')\n",
        "data_dir = 'drive/MyDrive/ct-bert/data/finetune/'+run_name+'/crowdbreaks'\n",
        "output_dir = 'drive/MyDrive/ct-bert/data/finetune/'+run_name+'/crowdbreaks'\n",
        "\n",
        "output_dir"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/ct-bert/data/finetune/run_2021-11-24_07-56-06_744272_test_run/crowdbreaks'"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KFlWFt5K7BY"
      },
      "source": [
        "def get_model_config(config_path):\n",
        "    config = bert_configs.BertConfig.from_json_file(config_path)\n",
        "    return config"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Aisf7mHyLHLc",
        "outputId": "2ffafa90-0a71-444e-e143-f4d551367c82"
      },
      "source": [
        "PRETRAINED_MODELS[args.model_class]['config']"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bert_config_large_uncased_wwm.json'"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnDO4WqHKzuw"
      },
      "source": [
        "def get_model_config_path(args):\n",
        "    try:\n",
        "        config_path = PRETRAINED_MODELS[args.model_class]['config']\n",
        "    except KeyError:\n",
        "        raise ValueError(f'Could not find a pretrained model matching the model class {args.model_class}')\n",
        "    return os.path.join('drive/MyDrive/ct-bert/configs/', config_path)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRoRxKboW1px"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1DyATGDKvn2"
      },
      "source": [
        "# Get configs\n",
        "pretrained_model_config_path = get_model_config_path(args)\n",
        "model_config = get_model_config(pretrained_model_config_path)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p7UwYyzDNU6z",
        "outputId": "6d620067-b553-4034-d6e2-8ca020624a55"
      },
      "source": [
        "pretrained_model_config_path"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/ct-bert/configs/bert_config_large_uncased_wwm.json'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNQn3y3UNS8I",
        "outputId": "a322cec5-d84f-4310-d1bd-d39ffaca35e3"
      },
      "source": [
        "model_config"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<official.nlp.bert.configs.BertConfig at 0x7fb438deaa90>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uNzLc6CNulB"
      },
      "source": [
        "def get_input_meta_data(data_dir):\n",
        "    with tf.io.gfile.GFile(data_dir+'/meta.json', 'rb') as reader:\n",
        "        input_meta_data = json.loads(reader.read().decode('utf-8'))\n",
        "    return input_meta_data\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb57DvopTQza"
      },
      "source": [
        "def get_label_mapping(data_dir):\n",
        "    with tf.io.gfile.GFile(data_dir+'/label_mapping.json', 'rb') as reader:\n",
        "        label_mapping = json.loads(reader.read().decode('utf-8'))\n",
        "    label_mapping = dict(zip(range(len(label_mapping)), label_mapping))\n",
        "    return label_mapping"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxHRsRtYN1S8"
      },
      "source": [
        "# Meta data/label mapping\n",
        "input_meta_data = get_input_meta_data(data_dir)\n",
        "label_mapping = get_label_mapping(data_dir)\n",
        "#logger.info(f'Loaded training data meta.json file: {input_meta_data}')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfjFRhd2T4Cz",
        "outputId": "bf20921c-8c33-4f7d-b64b-c4036667444d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_meta_data"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_data_size': 680,\n",
              " 'max_seq_length': 96,\n",
              " 'num_labels': 3,\n",
              " 'processor_type': 'text-classification',\n",
              " 'task_type': 'bert_classification',\n",
              " 'train_data_size': 2041}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx-OiPqJT6K7",
        "outputId": "948e60a7-f5ed-4d98-d42e-252a71e58570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "label_mapping"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: -1, 1: 0, 2: 1}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIakRXGsTpm1"
      },
      "source": [
        "# Calculate steps, warmup steps and eval steps\n",
        "train_data_size = input_meta_data['train_data_size']\n",
        "num_labels = input_meta_data['num_labels']\n",
        "max_seq_length = input_meta_data['max_seq_length']\n",
        "if args.limit_train_steps is None:\n",
        "    steps_per_epoch = int(train_data_size / args.train_batch_size)\n",
        "else:\n",
        "    steps_per_epoch = args.limit_train_steps\n",
        "warmup_steps = int(args.num_epochs * train_data_size * args.warmup_proportion/ args.train_batch_size)\n",
        "if args.limit_eval_steps is None:\n",
        "    eval_steps = int(math.ceil(input_meta_data['eval_data_size'] / args.eval_batch_size))\n",
        "else:\n",
        "    eval_steps = args.limit_eval_steps"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqh7nXarUspT"
      },
      "source": [
        "def get_model(args, model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length, is_hub_module=False):\n",
        "    # Get classifier and core model (used to initialize from checkpoint)\n",
        "    if args.init_checkpoint is None and PRETRAINED_MODELS[args.model_class]['is_tfhub_model']:\n",
        "        # load pretrained model from TF-hub\n",
        "        hub_module_url = f\"https://tfhub.dev/{PRETRAINED_MODELS[args.model_class]['hub_url']}\"\n",
        "        hub_module_trainable = True\n",
        "    else:\n",
        "        hub_module_url = None\n",
        "        hub_module_trainable = False\n",
        "    classifier_model, core_model = bert_models.classifier_model(\n",
        "            model_config,\n",
        "            num_labels,\n",
        "            max_seq_length,\n",
        "            hub_module_url=hub_module_url,\n",
        "            hub_module_trainable=hub_module_trainable)\n",
        "    # Optimizer\n",
        "    optimizer = utils.optimizer.create_optimizer(\n",
        "            args.learning_rate,\n",
        "            steps_per_epoch * args.num_epochs,\n",
        "            warmup_steps,\n",
        "            args.end_lr,\n",
        "            args.optimizer_type)\n",
        "    classifier_model.optimizer = configure_optimizer(\n",
        "            optimizer,\n",
        "            use_float16=False,\n",
        "            use_graph_rewrite=False)\n",
        "    return classifier_model, core_model"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7ubHQoLVsrv"
      },
      "source": [
        "def configure_optimizer(optimizer, use_float16=False, use_graph_rewrite=False, loss_scale='dynamic'):\n",
        "    \"\"\"Configures optimizer object with performance options.\"\"\"\n",
        "    if use_float16:\n",
        "        # Wraps optimizer with a LossScaleOptimizer. This is done automatically in compile() with the\n",
        "        # \"mixed_float16\" policy, but since we do not call compile(), we must wrap the optimizer manually.\n",
        "        optimizer = (tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, loss_scale=loss_scale))\n",
        "    if use_graph_rewrite:\n",
        "        # Note: the model dtype must be 'float32', which will ensure\n",
        "        # tf.ckeras.mixed_precision and tf.train.experimental.enable_mixed_precision_graph_rewrite do not double up.\n",
        "        optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(optimizer)\n",
        "    return optimizer"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLLj9CAfV4B_"
      },
      "source": [
        "def get_loss_fn(num_classes):\n",
        "    \"\"\"Gets the classification loss function.\"\"\"\n",
        "    def classification_loss_fn(labels, logits):\n",
        "        \"\"\"Classification loss.\"\"\"\n",
        "        labels = tf.squeeze(labels)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        one_hot_labels = tf.one_hot(tf.cast(labels, dtype=tf.int32), depth=num_classes, dtype=tf.float32)\n",
        "        per_example_loss = -tf.reduce_sum(tf.cast(one_hot_labels, dtype=tf.float32) * log_probs, axis=-1)\n",
        "        return tf.reduce_mean(per_example_loss)\n",
        "    return classification_loss_fn"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IpUfL8eUaTx"
      },
      "source": [
        "# Get model\n",
        "classifier_model, core_model = get_model(args, model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length)\n",
        "optimizer = classifier_model.optimizer\n",
        "loss_fn = get_loss_fn(num_labels)\n",
        "#try:\n",
        "#    if ',' in args.validation_freq:\n",
        "#        validation_freq = args.validation_freq.split(',')\n",
        "#        validation_freq = [int(v) for v in validation_freq]\n",
        "#    else:\n",
        "validation_freq = int(args.validation_freq)\n",
        "#except:\n",
        "#    raise ValueError(f'Invalid argument for validation_freq!')\n",
        "#logger.info(f'Using a validation frequency of {validation_freq}')\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrbklHJ-WtmZ",
        "outputId": "1688f599-55b6-4358-8270-7f64a7b6b849"
      },
      "source": [
        "classifier_model.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1024)         0           keras_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 3)            3075        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 335,144,964\n",
            "Trainable params: 335,144,963\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btcpefmyWazI",
        "outputId": "b8913fa6-19c5-423a-b55c-09e206f30db7"
      },
      "source": [
        "classifier_model.get_config()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_layers': {'input_mask': ['input_mask', 0, 0],\n",
              "  'input_type_ids': ['input_type_ids', 0, 0],\n",
              "  'input_word_ids': ['input_word_ids', 0, 0]},\n",
              " 'layers': [{'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 96),\n",
              "    'dtype': 'int32',\n",
              "    'name': 'input_word_ids',\n",
              "    'ragged': False,\n",
              "    'sparse': False},\n",
              "   'inbound_nodes': [],\n",
              "   'name': 'input_word_ids'},\n",
              "  {'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 96),\n",
              "    'dtype': 'int32',\n",
              "    'name': 'input_mask',\n",
              "    'ragged': False,\n",
              "    'sparse': False},\n",
              "   'inbound_nodes': [],\n",
              "   'name': 'input_mask'},\n",
              "  {'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 96),\n",
              "    'dtype': 'int32',\n",
              "    'name': 'input_type_ids',\n",
              "    'ragged': False,\n",
              "    'sparse': False},\n",
              "   'inbound_nodes': [],\n",
              "   'name': 'input_type_ids'},\n",
              "  {'class_name': 'KerasLayer',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'handle': 'https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/2',\n",
              "    'name': 'keras_layer',\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['input_word_ids', 0, 0, {}],\n",
              "     ['input_mask', 0, 0, {}],\n",
              "     ['input_type_ids', 0, 0, {}]]],\n",
              "   'name': 'keras_layer'},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.1,\n",
              "    'seed': None,\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['keras_layer', 0, 0, {}]]],\n",
              "   'name': 'dropout'},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'linear',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'TruncatedNormal',\n",
              "     'config': {'mean': 0.0, 'seed': None, 'stddev': 0.02}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'output',\n",
              "    'trainable': True,\n",
              "    'units': 3,\n",
              "    'use_bias': True},\n",
              "   'inbound_nodes': [[['dropout', 0, 0, {}]]],\n",
              "   'name': 'output'}],\n",
              " 'name': 'model',\n",
              " 'output_layers': [['output', 0, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmQ_fvsOXKaY"
      },
      "source": [
        "def get_metrics():\n",
        "    return [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sbmcj30W_hD"
      },
      "source": [
        "# Run keras compile\n",
        "#logger.info(f'Compiling keras model...')\n",
        "classifier_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=get_metrics())\n",
        "#logger.info(f'... done')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KplnImHSXPU5"
      },
      "source": [
        "# Create all custom callbacks\n",
        "summary_dir = os.path.join(output_dir, 'summaries')\n",
        "summary_callback = tf.keras.callbacks.TensorBoard(summary_dir, profile_batch=0)\n",
        "time_history_callback = keras_utils.TimeHistory(\n",
        "    batch_size=args.train_batch_size,\n",
        "    log_steps=args.time_history_log_steps,\n",
        "    logdir=summary_dir)\n",
        "custom_callbacks = [summary_callback, time_history_callback]\n",
        "if args.save_model:\n",
        "    #logger.info('Using save_model option...')\n",
        "    checkpoint_path = os.path.join(output_dir, 'checkpoint')\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
        "    custom_callbacks.append(checkpoint_callback)\n",
        "if args.early_stopping_epochs > 0:\n",
        "    #logger.info(f'Using early stopping of after {args.early_stopping_epochs} epochs of val_loss not decreasing')\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=args.early_stopping_epochs, monitor='val_loss')\n",
        "    custom_callbacks.append(early_stopping_callback)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoCfM5AVYqxL"
      },
      "source": [
        "def get_dataset_fn(input_file_pattern, max_seq_length, global_batch_size, is_training=True):\n",
        "  \"\"\"Gets a closure to create a dataset.\"\"\"\n",
        "  def _dataset_fn(ctx=None):\n",
        "    \"\"\"Returns tf.data.Dataset for distributed BERT pretraining.\"\"\"\n",
        "    batch_size = ctx.get_per_replica_batch_size(\n",
        "        global_batch_size) if ctx else global_batch_size\n",
        "    dataset = input_pipeline.create_classifier_dataset(\n",
        "        input_file_pattern,\n",
        "        max_seq_length,\n",
        "        batch_size,\n",
        "        is_training=is_training,\n",
        "        input_pipeline_context=ctx)\n",
        "    return dataset\n",
        "\n",
        "  return _dataset_fn"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdiP41HIYi0d"
      },
      "source": [
        "# Generate dataset_fn\n",
        "train_input_fn = get_dataset_fn(\n",
        "    data_dir+'/tfrecords/train.tfrecords',\n",
        "    max_seq_length,\n",
        "    args.train_batch_size,\n",
        "    is_training=True)\n",
        "eval_input_fn = get_dataset_fn(\n",
        "    data_dir+'/tfrecords/dev.tfrecords',\n",
        "    max_seq_length,\n",
        "    args.eval_batch_size,\n",
        "    is_training=False)\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dk8jrhOY0_x"
      },
      "source": [
        "# Add mertrics callback to calculate performance metrics at the end of epoch\n",
        "performance_metrics_callback = Metrics(\n",
        "        eval_input_fn,\n",
        "        label_mapping,\n",
        "        os.path.join(summary_dir, 'metrics'),\n",
        "        eval_steps,\n",
        "        args.eval_batch_size,\n",
        "        validation_freq)\n",
        "custom_callbacks.append(performance_metrics_callback)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klX424ubY-fw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "117a1242-6c8f-466a-b104-e33804820718"
      },
      "source": [
        "# Run keras fit\n",
        "time_start = time.time()\n",
        "#logger.info('Run training...')\n",
        "history = classifier_model.fit(\n",
        "    x=train_input_fn(),\n",
        "    validation_data=eval_input_fn(),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=args.num_epochs,\n",
        "    validation_steps=eval_steps,\n",
        "    validation_freq=validation_freq,\n",
        "    callbacks=custom_callbacks,\n",
        "    verbose=1)\n",
        "time_end = time.time()\n",
        "training_time_min = (time_end-time_start)/60\n",
        "#logger.info(f'Finished training after {training_time_min:.1f} min')\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255/255 [==============================] - ETA: 0s - loss: 0.7549 - accuracy: 0.6740\n",
            "Epoch 00001: saving model to drive/MyDrive/ct-bert/data/finetune/run_2021-11-24_07-56-06_744272_test_run/crowdbreaks/checkpoint\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1985\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: drive/MyDrive/ct-bert/data/finetune/run_2021-11-24_07-56-06_744272_test_run/crowdbreaks/dev.tfrecords; No such file or directory [Op:IteratorGetNext]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-7af170e9c8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     verbose=1)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtime_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtraining_time_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/ct-bert/utils/finetune_helpers.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Computing metrics on validation set...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mt_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/ct-bert/utils/finetune_helpers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Computing metrics on validation set...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mt_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: drive/MyDrive/ct-bert/data/finetune/run_2021-11-24_07-56-06_744272_test_run/crowdbreaks/dev.tfrecords; No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UofuqpqPijJv",
        "outputId": "bfa284cc-a97f-402f-8652-53ff9806ec96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "# Write training log\n",
        "all_scores = performance_metrics_callback.scores\n",
        "all_predictions = performance_metrics_callback.predictions\n",
        "if len(all_scores) > 0:\n",
        "    final_scores = all_scores[-1]\n",
        "    #logger.info(f'Final eval scores: {final_scores}')\n",
        "else:\n",
        "    final_scores = {}\n",
        "full_history = history.history\n",
        "if len(full_history) > 0:\n",
        "    final_val_loss = full_history['val_loss'][-1]\n",
        "    final_loss = full_history['loss'][-1]\n",
        "    #logger.info(f'Final training loss: {final_loss:.2f}, Final validation loss: {final_val_loss:.2f}')\n",
        "else:\n",
        "    final_val_loss = None\n",
        "    final_loss = None\n",
        "data = {\n",
        "        'created_at': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'run_name': run_name,\n",
        "        'final_loss': final_loss,\n",
        "        'final_val_loss': final_val_loss,\n",
        "        'max_seq_length': max_seq_length,\n",
        "        'num_train_steps': steps_per_epoch * args.num_epochs,\n",
        "        'eval_steps': eval_steps,\n",
        "        'steps_per_epoch': steps_per_epoch,\n",
        "        'training_time_min': training_time_min,\n",
        "        'data_dir': data_dir,\n",
        "        'output_dir': output_dir,\n",
        "        'all_scores': all_scores,\n",
        "        'all_predictions': all_predictions,\n",
        "        'num_labels': num_labels,\n",
        "        'label_mapping': label_mapping,\n",
        "        **full_history,\n",
        "        **final_scores,\n",
        "        'args':args}"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-00b3a7696a04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfinal_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfull_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_history\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfinal_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2D_-idSUpOAO",
        "outputId": "e8a6644c-a895-475d-8f06-66a65512cec8"
      },
      "source": [
        "os.path.join(output_dir, run_name, 'run_logs.json')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/vax_bert_output/run_2021-11-19_14-17-34_470901/_run_logs.json'"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdzEJlDYjB-c"
      },
      "source": [
        "# Write run_log\n",
        "f_path_training_log = os.path.join(output_dir, run_name, '_run_logs.json')\n",
        "#logger.info(f'Writing training log to {f_path_training_log}...')\n",
        "save_to_json(data, f_path_training_log)\n",
        "# Write bert config\n",
        "model_config.id2label = label_mapping\n",
        "model_config.label2id = {v:k for k, v in label_mapping.items()}\n",
        "model_config.max_seq_length = max_seq_length\n",
        "model_config.num_labels = num_labels\n",
        "f_path_bert_config = os.path.join(output_dir,  run_name, 'bert_config.json')\n",
        "#logger.info(f'Writing BERT config to {f_path_bert_config}...')\n",
        "save_to_json(model_config.to_dict(), f_path_bert_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saG4Www4bmUF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}