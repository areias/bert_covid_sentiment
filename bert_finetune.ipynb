{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_finetune.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNOo1mooqfefTkFalhHQiHW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/areias/bert_covid_sentiment/blob/main/bert_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc1lcmJX1zZY"
      },
      "source": [
        "## Training\n",
        "\n",
        "from https://github.com/digitalepidemiologylab/covid-twitter-bert/blob/c87912b409659f40018e839c4124be5ae2486713/run_finetune.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3pnT0gk1OkH",
        "outputId": "af8f65b0-ccda-4a0b-dd33-a96376fb68c3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38g5ufj-LXcu",
        "outputId": "b7086c28-c90d-46e9-98ca-407fc7c25901"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "covid-twitter-bert  drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR5d6VJH1VPD",
        "outputId": "66d17239-6b95-4c14-b6bf-5ea209114cc3"
      },
      "source": [
        "!git clone -b master https://github.com/digitalepidemiologylab/covid-twitter-bert.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'covid-twitter-bert'...\n",
            "remote: Enumerating objects: 1660, done.\u001b[K\n",
            "remote: Counting objects: 100% (222/222), done.\u001b[K\n",
            "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
            "remote: Total 1660 (delta 150), reused 140 (delta 88), pack-reused 1438\u001b[K\n",
            "Receiving objects: 100% (1660/1660), 3.48 MiB | 14.79 MiB/s, done.\n",
            "Resolving deltas: 100% (1050/1050), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FviqvxIN2ADC",
        "outputId": "34125fcb-1a60-4bb5-f441-7ff8493778c7"
      },
      "source": [
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 4.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.12.0)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 29.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.17.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.19.5)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 35.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.13.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.41.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.6.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onLVqWna4Jra",
        "outputId": "6eaa1e19-71b7-4192-a0b6-71537ebc2731"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBa7T2m_2x2Y",
        "outputId": "7ac271b5-72d7-4650-def9-cf6ba3a8d0e5"
      },
      "source": [
        "!pip install  tensorflow_addons==0.11.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons==0.11.2\n",
            "  Downloading tensorflow_addons-0.11.2-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons==0.11.2) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "UUYILnDx2aiD",
        "outputId": "a845e6c6-206c-48d3-c654-713fd0c27de4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN8GlUhz4snY"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/models-93490036e00f37ecbe6693b9ff4ae488bb8e9270')\n",
        "sys.path.append('/content/covid-twitter-bert')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr_erlWn4pRF"
      },
      "source": [
        "from official.nlp.bert import bert_models\n",
        "from official.utils.misc import distribution_utils\n",
        "from official.nlp.bert import configs as bert_configs\n",
        "from official.modeling import performance\n",
        "from official.nlp.bert import input_pipeline\n",
        "from official.utils.misc import keras_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpIFoL2t6Gg9"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "import logging\n",
        "from logging.handlers import RotatingFileHandler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scrn_g3e7dnt"
      },
      "source": [
        "import tqdm\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from utils.misc import ArgParseDefault, save_to_json, add_bool_arg\n",
        "from utils.finetune_helpers import Metrics\n",
        "import utils.optimizer\n",
        "from config import PRETRAINED_MODELS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHbR3Xha9SlI"
      },
      "source": [
        "import datetime\n",
        "def get_run_name():\n",
        "    # Use timestamp to generate a unique run name\n",
        "    ts = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S_%f')\n",
        "    run_name = f'run_{ts}'\n",
        "    return run_name\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "emCDNNzY_D4c",
        "outputId": "fad81b97-828d-444e-90b7-b89f67123ac7"
      },
      "source": [
        "\"\"\"python run_finetune.py \\\n",
        "  --run_prefix $RUN_PREFIX \\\n",
        "  --bucket_name $BUCKET_NAME \\\n",
        "  --tpu_ip $TPU_IP \\\n",
        "  --model_class $MODEL_CLASS \\\n",
        "  --finetune_data ${FINETUNE_DATA}/${FINETUNE_DATASET} \\\n",
        "  --train_batch_size $TRAIN_BATCH_SIZE \\\n",
        "  --eval_batch_size $EVAL_BATCH_SIZE \\\n",
        "  --num_epochs $NUM_EPOCHS \\\n",
        "  --learning_rate $LR\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'python run_finetune.py   --run_prefix $RUN_PREFIX   --bucket_name $BUCKET_NAME   --tpu_ip $TPU_IP   --model_class $MODEL_CLASS   --finetune_data ${FINETUNE_DATA}/${FINETUNE_DATASET}   --train_batch_size $TRAIN_BATCH_SIZE   --eval_batch_size $EVAL_BATCH_SIZE   --num_epochs $NUM_EPOCHS   --learning_rate $LR'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smsaDoCX-_pV"
      },
      "source": [
        "# args\n",
        "\n",
        "RUN_PREFIX='testrun'                                  # Name your run\n",
        "#BUCKET_NAME=                                        # Fill in your buckets name here (without the gs:// prefix)\n",
        "#TPU_IP=XX.XX.XXX.X                                  # Fill in your TPUs IP here\n",
        "FINETUNE_DATASET='crowdbreaks'                      # Your dataset name\n",
        "FINETUNE_DATA= get_run_name()                         # Fill in dataset run name (e.g. run_2020-05-19_14-14-53_517063_test_run)\n",
        "MODEL_CLASS='covid-twitter-bert'\n",
        "TRAIN_BATCH_SIZE=8 #32\n",
        "EVAL_BATCH_SIZE=8\n",
        "LR=2e-5\n",
        "NUM_EPOCHS=1"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEkHqvhF7xef",
        "outputId": "575468d0-1b3d-428d-b3b4-32f216947c71"
      },
      "source": [
        "from collections import namedtuple\n",
        "arguments = namedtuple('arguments', ['run_prefix','model_class','finetune_data', \n",
        "                                     'train_batch_size', 'eval_batch_size','learning_rate',\n",
        "                                     'limit_train_steps','limit_eval_steps','num_epochs',\n",
        "                                     'warmup_proportion', 'init_checkpoint','validation_freq',\n",
        "                                     'end_lr','optimizer_type', 'save_model',\n",
        "                                     'early_stopping_epochs','time_history_log_steps'])\n",
        "\n",
        "args = arguments(RUN_PREFIX,MODEL_CLASS,FINETUNE_DATA,\n",
        "                 TRAIN_BATCH_SIZE,EVAL_BATCH_SIZE,LR,\n",
        "                 None,None,1,\n",
        "                 0.1, None,1,\n",
        "                 0,'adamw',True,\n",
        "                1,10)            \n",
        "args "
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "arguments(run_prefix='testrun', model_class='covid-twitter-bert', finetune_data='run_2021-11-19_13-34-32_104134', train_batch_size=8, eval_batch_size=8, learning_rate=2e-05, limit_train_steps=None, limit_eval_steps=None, num_epochs=1, warmup_proportion=0.1, init_checkpoint=None, validation_freq=1, end_lr=0, optimizer_type='adamw', save_model=True, early_stopping_epochs=1, time_history_log_steps=10)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoAbELtb8PMn"
      },
      "source": [
        "def set_mixed_precision_policy(args):\n",
        "    \"\"\"Sets mix precision policy.\"\"\"\n",
        "    if args.dtype == 'fp16':\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16', loss_scale=loss_scale)\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "    elif args.dtype == 'bf16':\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "    elif args.dtype == 'fp32':\n",
        "        tf.keras.mixed_precision.experimental.set_policy('float32')\n",
        "    else:\n",
        "        raise ValueError(f'Unknown dtype {args.dtype}')"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUseChVU_kpN"
      },
      "source": [
        "#def run(args):\n",
        "\"\"\"Train using the Keras/TF 2.0. Adapted from the tensorflow/models Github\"\"\"\n",
        "# CONFIG\n",
        "run_name = get_run_name()\n",
        "#logger.info(f'*** Starting run {run_name} ***')\n",
        "data_dir = 'drive/MyDrive/vax-bert-data'\n",
        "output_dir = 'drive/MyDrive/vax-bert-output'\n",
        "\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KFlWFt5K7BY"
      },
      "source": [
        "def get_model_config(config_path):\n",
        "    config = bert_configs.BertConfig.from_json_file(config_path)\n",
        "    return config"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Aisf7mHyLHLc",
        "outputId": "7a9cbc28-dc69-430c-c6ae-7eec1f51cb85"
      },
      "source": [
        "PRETRAINED_MODELS[args.model_class]['config']"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bert_config_covid_twitter_bert.json'"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdGZg57FMce3",
        "outputId": "30ca8640-3323-425a-e0e7-d553379e1f1a"
      },
      "source": [
        "!ls covid-twitter-bert/configs"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_config_covid_twitter_bert.json  bert_config_large_uncased_wwm.json\n",
            "bert_config_large_uncased.json\t     bert_config_multi_cased.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnDO4WqHKzuw"
      },
      "source": [
        "def get_model_config_path(args):\n",
        "    try:\n",
        "        config_path = PRETRAINED_MODELS[args.model_class]['config']\n",
        "    except KeyError:\n",
        "        raise ValueError(f'Could not find a pretrained model matching the model class {args.model_class}')\n",
        "    return os.path.join('covid-twitter-bert/configs/', config_path)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1DyATGDKvn2"
      },
      "source": [
        "# Get configs\n",
        "pretrained_model_config_path = get_model_config_path(args)\n",
        "model_config = get_model_config(pretrained_model_config_path)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p7UwYyzDNU6z",
        "outputId": "4f1eec1f-0f6f-44b2-8f20-918bb4bed9c0"
      },
      "source": [
        "pretrained_model_config_path"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'covid-twitter-bert/configs/bert_config_covid_twitter_bert.json'"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNQn3y3UNS8I",
        "outputId": "e8d8e08a-cab4-4aaa-82f6-91f7367c364e"
      },
      "source": [
        "model_config"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<official.nlp.bert.configs.BertConfig at 0x7f5966997210>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1dni-qwMR8Uk",
        "outputId": "c272ce61-059e-4bf3-cd37-47fec698f995"
      },
      "source": [
        "os.path.join(data_dir, 'meta.json')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/vax-bert-data/meta.json'"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YA848vNS92F",
        "outputId": "cdbcaa2b-246d-4f4e-f5cc-89128aa81084"
      },
      "source": [
        "!ls drive/MyDrive/vax_bert_data"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create_finetune_config.json  label_mapping.json  train.tfrecords\n",
            "dev.tfrecords\t\t     meta.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uNzLc6CNulB"
      },
      "source": [
        "def get_input_meta_data(data_dir):\n",
        "    with tf.io.gfile.GFile('drive/MyDrive/vax_bert_data/meta.json', 'rb') as reader:\n",
        "        input_meta_data = json.loads(reader.read().decode('utf-8'))\n",
        "    return input_meta_data\n"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb57DvopTQza"
      },
      "source": [
        "def get_label_mapping(data_dir):\n",
        "    with tf.io.gfile.GFile('drive/MyDrive/vax_bert_data/label_mapping.json', 'rb') as reader:\n",
        "        label_mapping = json.loads(reader.read().decode('utf-8'))\n",
        "    label_mapping = dict(zip(range(len(label_mapping)), label_mapping))\n",
        "    return label_mapping"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxHRsRtYN1S8"
      },
      "source": [
        "# Meta data/label mapping\n",
        "input_meta_data = get_input_meta_data(data_dir)\n",
        "label_mapping = get_label_mapping(data_dir)\n",
        "#logger.info(f'Loaded training data meta.json file: {input_meta_data}')"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIakRXGsTpm1"
      },
      "source": [
        "# Calculate steps, warmup steps and eval steps\n",
        "train_data_size = input_meta_data['train_data_size']\n",
        "num_labels = input_meta_data['num_labels']\n",
        "max_seq_length = input_meta_data['max_seq_length']\n",
        "if args.limit_train_steps is None:\n",
        "    steps_per_epoch = int(train_data_size / args.train_batch_size)\n",
        "else:\n",
        "    steps_per_epoch = args.limit_train_steps\n",
        "warmup_steps = int(args.num_epochs * train_data_size * args.warmup_proportion/ args.train_batch_size)\n",
        "if args.limit_eval_steps is None:\n",
        "    eval_steps = int(math.ceil(input_meta_data['eval_data_size'] / args.eval_batch_size))\n",
        "else:\n",
        "    eval_steps = args.limit_eval_steps"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqh7nXarUspT"
      },
      "source": [
        "def get_model(args, model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length, is_hub_module=False):\n",
        "    # Get classifier and core model (used to initialize from checkpoint)\n",
        "    if args.init_checkpoint is None and PRETRAINED_MODELS[args.model_class]['is_tfhub_model']:\n",
        "        # load pretrained model from TF-hub\n",
        "        hub_module_url = f\"https://tfhub.dev/{PRETRAINED_MODELS[args.model_class]['hub_url']}\"\n",
        "        hub_module_trainable = True\n",
        "    else:\n",
        "        hub_module_url = None\n",
        "        hub_module_trainable = False\n",
        "    classifier_model, core_model = bert_models.classifier_model(\n",
        "            model_config,\n",
        "            num_labels,\n",
        "            max_seq_length,\n",
        "            hub_module_url=hub_module_url,\n",
        "            hub_module_trainable=hub_module_trainable)\n",
        "    # Optimizer\n",
        "    optimizer = utils.optimizer.create_optimizer(\n",
        "            args.learning_rate,\n",
        "            steps_per_epoch * args.num_epochs,\n",
        "            warmup_steps,\n",
        "            args.end_lr,\n",
        "            args.optimizer_type)\n",
        "    classifier_model.optimizer = configure_optimizer(\n",
        "            optimizer,\n",
        "            use_float16=False,\n",
        "            use_graph_rewrite=False)\n",
        "    return classifier_model, core_model"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7ubHQoLVsrv"
      },
      "source": [
        "def configure_optimizer(optimizer, use_float16=False, use_graph_rewrite=False, loss_scale='dynamic'):\n",
        "    \"\"\"Configures optimizer object with performance options.\"\"\"\n",
        "    if use_float16:\n",
        "        # Wraps optimizer with a LossScaleOptimizer. This is done automatically in compile() with the\n",
        "        # \"mixed_float16\" policy, but since we do not call compile(), we must wrap the optimizer manually.\n",
        "        optimizer = (tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, loss_scale=loss_scale))\n",
        "    if use_graph_rewrite:\n",
        "        # Note: the model dtype must be 'float32', which will ensure\n",
        "        # tf.ckeras.mixed_precision and tf.train.experimental.enable_mixed_precision_graph_rewrite do not double up.\n",
        "        optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(optimizer)\n",
        "    return optimizer"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLLj9CAfV4B_"
      },
      "source": [
        "def get_loss_fn(num_classes):\n",
        "    \"\"\"Gets the classification loss function.\"\"\"\n",
        "    def classification_loss_fn(labels, logits):\n",
        "        \"\"\"Classification loss.\"\"\"\n",
        "        labels = tf.squeeze(labels)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        one_hot_labels = tf.one_hot(tf.cast(labels, dtype=tf.int32), depth=num_classes, dtype=tf.float32)\n",
        "        per_example_loss = -tf.reduce_sum(tf.cast(one_hot_labels, dtype=tf.float32) * log_probs, axis=-1)\n",
        "        return tf.reduce_mean(per_example_loss)\n",
        "    return classification_loss_fn"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IpUfL8eUaTx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "c048fc1e-b587-4531-c63b-3f8dae539ae2"
      },
      "source": [
        "# Get model\n",
        "classifier_model, core_model = get_model(args, model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length)\n",
        "optimizer = classifier_model.optimizer\n",
        "loss_fn = get_loss_fn(num_labels)\n",
        "#try:\n",
        "#    if ',' in args.validation_freq:\n",
        "#        validation_freq = args.validation_freq.split(',')\n",
        "#        validation_freq = [int(v) for v in validation_freq]\n",
        "#    else:\n",
        "validation_freq = int(args.validation_freq)\n",
        "#except:\n",
        "#    raise ValueError(f'Invalid argument for validation_freq!')\n",
        "#logger.info(f'Using a validation frequency of {validation_freq}')\n"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-ff86870921ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-106-51925cfd566d>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(args, model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length, is_hub_module)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mhub_module_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             hub_module_trainable=hub_module_trainable)\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     optimizer = utils.optimizer.create_optimizer(\n",
            "\u001b[0;32m/content/drive/MyDrive/models-93490036e00f37ecbe6693b9ff4ae488bb8e9270/official/nlp/bert/bert_models.py\u001b[0m in \u001b[0;36mclassifier_model\u001b[0;34m(bert_config, num_labels, max_seq_length, final_layer_initializer, hub_module_url, hub_module_trainable)\u001b[0m\n\u001b[1;32m    311\u001b[0m       shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n\u001b[1;32m    312\u001b[0m   bert_model = hub.KerasLayer(\n\u001b[0;32m--> 313\u001b[0;31m       hub_module_url, trainable=hub_module_trainable)\n\u001b[0m\u001b[1;32m    314\u001b[0m   \u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_word_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m   output = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[1;32m    105\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m   \"\"\"\n\u001b[0;32m--> 578\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, loader_cls)\u001b[0m\n\u001b[1;32m    602\u001b[0m       loader = loader_cls(object_graph_proto,\n\u001b[1;32m    603\u001b[0m                           \u001b[0msaved_model_proto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m                           export_dir)\n\u001b[0m\u001b[1;32m    605\u001b[0m       \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mload_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m       \u001b[0mload_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_existing_objects_matched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         graph_view=self._graph_view)\n\u001b[1;32m   1303\u001b[0m     base.CheckpointPosition(\n\u001b[0;32m-> 1304\u001b[0;31m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[0m\u001b[1;32m   1305\u001b[0m     load_status = CheckpointLoadStatus(\n\u001b[1;32m   1306\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, trackable)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[0;34m(self, checkpoint_position)\u001b[0m\n\u001b[1;32m    905\u001b[0m     restore_ops.extend(\n\u001b[1;32m    906\u001b[0m         current_position.checkpoint.restore_saveables(\n\u001b[0;32m--> 907\u001b[0;31m             tensor_saveables, python_saveables))\n\u001b[0m\u001b[1;32m    908\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[0;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[1;32m    287\u001b[0m              \"expecting %s\") % (tensor_saveables.keys(), validated_names))\n\u001b[1;32m    288\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver(\n\u001b[0;32m--> 289\u001b[0;31m           validated_saveables).restore(self.save_path_tensor)\n\u001b[0m\u001b[1;32m    290\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_device_savers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_restore_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix)\u001b[0m\n\u001b[1;32m    101\u001b[0m                                           structured_restored_tensors):\n\u001b[1;32m    102\u001b[0m       restore_ops[saveable.name] = saveable.restore(\n\u001b[0;32m--> 103\u001b[0;31m           restored_tensors, restored_shapes=None)\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Copy the restored tensor to the variable's device.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m       \u001b[0mrestored_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m       return resource_variable_ops.shape_safe_assign_variable_handle(\n\u001b[1;32m    116\u001b[0m           self.handle_op, self._var_shape, restored_tensor)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   3896\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3898\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3899\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3900\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrbklHJ-WtmZ"
      },
      "source": [
        "classifier_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btcpefmyWazI"
      },
      "source": [
        "classifier_model.get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmQ_fvsOXKaY"
      },
      "source": [
        "def get_metrics():\n",
        "    return [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sbmcj30W_hD"
      },
      "source": [
        "# Run keras compile\n",
        "#logger.info(f'Compiling keras model...')\n",
        "classifier_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=get_metrics())\n",
        "#logger.info(f'... done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KplnImHSXPU5"
      },
      "source": [
        "# Create all custom callbacks\n",
        "summary_dir = os.path.join(output_dir, 'summaries')\n",
        "summary_callback = tf.keras.callbacks.TensorBoard(summary_dir, profile_batch=0)\n",
        "time_history_callback = keras_utils.TimeHistory(\n",
        "    batch_size=args.train_batch_size,\n",
        "    log_steps=args.time_history_log_steps,\n",
        "    logdir=summary_dir)\n",
        "custom_callbacks = [summary_callback, time_history_callback]\n",
        "if args.save_model:\n",
        "    #logger.info('Using save_model option...')\n",
        "    checkpoint_path = os.path.join(output_dir, 'checkpoint')\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
        "    custom_callbacks.append(checkpoint_callback)\n",
        "if args.early_stopping_epochs > 0:\n",
        "    #logger.info(f'Using early stopping of after {args.early_stopping_epochs} epochs of val_loss not decreasing')\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=args.early_stopping_epochs, monitor='val_loss')\n",
        "    custom_callbacks.append(early_stopping_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoCfM5AVYqxL"
      },
      "source": [
        "def get_dataset_fn(input_file_pattern, max_seq_length, global_batch_size, is_training=True):\n",
        "  \"\"\"Gets a closure to create a dataset.\"\"\"\n",
        "  def _dataset_fn(ctx=None):\n",
        "    \"\"\"Returns tf.data.Dataset for distributed BERT pretraining.\"\"\"\n",
        "    batch_size = ctx.get_per_replica_batch_size(\n",
        "        global_batch_size) if ctx else global_batch_size\n",
        "    dataset = input_pipeline.create_classifier_dataset(\n",
        "        input_file_pattern,\n",
        "        max_seq_length,\n",
        "        batch_size,\n",
        "        is_training=is_training,\n",
        "        input_pipeline_context=ctx)\n",
        "    return dataset\n",
        "\n",
        "  return _dataset_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdiP41HIYi0d"
      },
      "source": [
        "# Generate dataset_fn\n",
        "train_input_fn = get_dataset_fn(\n",
        "    os.path.join(data_dir, 'tfrecords', 'train.tfrecords'),\n",
        "    max_seq_length,\n",
        "    args.train_batch_size,\n",
        "    is_training=True)\n",
        "eval_input_fn = get_dataset_fn(\n",
        "    os.path.join(data_dir, 'tfrecords', 'dev.tfrecords'),\n",
        "    max_seq_length,\n",
        "    args.eval_batch_size,\n",
        "    is_training=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dk8jrhOY0_x"
      },
      "source": [
        "# Add mertrics callback to calculate performance metrics at the end of epoch\n",
        "performance_metrics_callback = Metrics(\n",
        "        eval_input_fn,\n",
        "        label_mapping,\n",
        "        os.path.join(summary_dir, 'metrics'),\n",
        "        eval_steps,\n",
        "        args.eval_batch_size,\n",
        "        validation_freq)\n",
        "custom_callbacks.append(performance_metrics_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klX424ubY-fw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21be5a42-1231-4204-af88-be4703699dcc"
      },
      "source": [
        "# Run keras fit\n",
        "time_start = time.time()\n",
        "#logger.info('Run training...')\n",
        "history = classifier_model.fit(\n",
        "    x=train_input_fn(),\n",
        "    validation_data=eval_input_fn(),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=args.num_epochs,\n",
        "    validation_steps=eval_steps,\n",
        "    validation_freq=validation_freq,\n",
        "    callbacks=custom_callbacks,\n",
        "    verbose=1)\n",
        "time_end = time.time()\n",
        "training_time_min = (time_end-time_start)/60\n",
        "#logger.info(f'Finished training after {training_time_min:.1f} min')\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-7af170e9c8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     verbose=1)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtime_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtraining_time_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:541 train_step  **\n        self.trainable_variables)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1812 _minimize\n        experimental_aggregate_gradients=False)\n    /content/covid-twitter-bert/utils/optimizer.py:178 apply_gradients\n        experimental_aggregate_gradients=experimental_aggregate_gradients)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:478 apply_gradients\n        self._create_all_weights(var_list)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:663 _create_all_weights\n        self._create_slots(var_list)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/adam.py:158 _create_slots\n        self.add_slot(var, 'v')\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:723 add_slot\n        initial_value=initial_value)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py:261 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py:255 _variable_v2_call\n        shape=shape)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py:66 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2562 creator\n        return next_creator(**kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py:66 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2562 creator\n        return next_creator(**kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py:66 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2562 creator\n        return next_creator(**kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py:66 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:494 variable_capturing_scope\n        lifted_initializer_graph=lifted_initializer_graph, **kwds)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py:263 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:180 __init__\n        initial_value() if init_from_fn else initial_value,\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/init_ops_v2.py:129 __call__\n        return array_ops.zeros(shape, dtype)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:2677 wrapped\n        tensor = fun(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:2733 zeros\n        output = fill(shape, constant(zero, dtype=dtype), name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:234 fill\n        result = gen_array_ops.fill(dims, value, name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py:3316 fill\n        _ops.raise_from_not_ok_status(e, name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:6653 raise_from_not_ok_status\n        six.raise_from(core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[1024,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "wweeqBPN7eRW",
        "outputId": "7185ce4d-682d-4db0-c04d-90f4c3ae9aea"
      },
      "source": [
        "#def main(args):\n",
        "    # Set TF Hub caching to bucket\n",
        "    #os.environ['TFHUB_CACHE_DIR'] = os.path.join(f'gs://{args.bucket_name}/tmp')\n",
        "    # Get distribution strategy\n",
        "    #if args.use_tpu:\n",
        "    #    if args.tpu_ip:\n",
        "    #        logger.info(f'Intializing TPU on address {args.tpu_ip}...')\n",
        "    #        tpu_address = f'grpc://{args.tpu_ip}:8470'\n",
        "    #        strategy = distribution_utils.get_distribution_strategy(distribution_strategy='tpu', tpu_address=tpu_address)\n",
        "    #    elif args.tpu_name:\n",
        "    #        logger.info(f'Intializing TPU with name {args.tpu_name}...')\n",
        "    #        cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args.tpu_name)\n",
        "    #        tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "    #        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "    #        strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n",
        "    #    else:\n",
        "    #       raise ValueError(f'You need to either specify a tpu_ip or a tpu_name in order to use a TPU.')\n",
        "    #else:\n",
        "    #    strategy = distribution_utils.get_distribution_strategy(distribution_strategy='mirrored', num_gpus=args.num_gpus)\n",
        "# set mixed precision\n",
        "set_mixed_precision_policy(args)\n",
        "# Run training\n",
        "for repeat in range(args.repeats):\n",
        "    with strategy.scope():\n",
        "        run(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-2fe96691625a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#    strategy = distribution_utils.get_distribution_strategy(distribution_strategy='mirrored', num_gpus=args.num_gpus)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# set mixed precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mset_mixed_precision_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# Run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrepeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-faa1e74e50bd>\u001b[0m in \u001b[0;36mset_mixed_precision_policy\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mset_mixed_precision_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Sets mix precision policy.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fp16'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mixed_float16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'arguments' object has no attribute 'dtype'"
          ]
        }
      ]
    }
  ]
}